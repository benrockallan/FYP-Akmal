# STEP 0: TikTok Data Extraction and Processing

# Ensure necessary libraries are installed
!pip install apify-client tqdm openpyxl pandas

import os
import re
import time
import csv
import string
from pprint import pprint
from tqdm.auto import tqdm
import pandas as pd
from apify_client import ApifyClient
from IPython.display import display, clear_output

# --- Configuration ---
# IMPORTANT: Replace with your actual Apify API Token or ensure it's set as an environment variable
APIFY_API_TOKEN = os.getenv('APIFY_API_TOKEN', 'apify_api_lRuNeeqW3bt3EJMaav3mvk83IxOWiV3jgp18') # Replace placeholder if not using env var

# *** SET YOUR DESIRED OUTPUT DIRECTORY HERE ***
OUTPUT_DIRECTORY = r"G:\Other computers\My Laptop (1)\Pelajaran UiTM\Classes and Lectures (Semester 6)\CSP650\Developments\Simplified Data\Merged Files\Outputs\Apify Output" # <-- CHANGE THIS PATH IF NEEDED

# *** SET PATH TO YOUR LIST OF PROFILES ***
PROFILE_LIST_FILE = r"G:\Other computers\My Laptop (1)\Pelajaran UiTM\Classes and Lectures (Semester 6)\CSP650\Developments\Lists of Psychiatrist.txt" # <-- Path to your text file

# The user's query (Used for filtering results - can be adjusted)
user_input_query = "Why am I always unhappy? Like I can't get up from bed everyday"

#------------------------------------------------------#

# --- Scraping Parameters for Video Actor OtzYfK1ndEGdwWFKQ ---
VIDEO_ACTOR_INPUT_SETTINGS = {
    "resultsPerPage": 1, # How many videos to attempt per profile
    "shouldDownloadVideos": False,
    "shouldDownloadCovers": False,
    "shouldDownloadSubtitles": True, # Keep enabled to get transcription links
    "shouldDownloadSlideshowImages": False,
}

# --- Scraping Parameters for Comment Actor clockworks/tiktok-comments-scraper ---
COMMENTS_PER_POST_LIMIT = 15 # Number of comments to fetch per video
MAX_REPLIES_PER_COMMENT = 5  # Number of replies per comment (0 for none)

# --- Polling Parameters ---
POLL_INTERVAL_SECONDS = 10 # How often to check Apify run status
MAX_WAIT_MINUTES = 20      # Max time to wait for Apify runs

# --- Desired Output Columns ---
# Define the exact column names and order for the final CSV files
VIDEO_COLUMNS = ['Vid', 'Name', 'NickName', 'ProfileUrl', 'TikTok_Transcription', 'TikTok_Link', 'Author Bio', 'Video Description']
COMMENT_COLUMNS = ['Vid', 'Cid', 'RepliesToId', 'Comments', 'uniqueId', 'videoWebUrl']

# --- Helper Function: Load Profile USERNAMES from File ---
def load_profile_usernames(filepath):
    """ Reads a text file and extracts unique TikTok usernames from profile URLs or lines. """
    usernames = set()
    print(f"Reading profile URLs/Usernames from: {filepath}")
    try:
        # Try UTF-8 first, then fallback
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                lines = f.readlines()
        except UnicodeDecodeError:
            print("   Warning: UTF-8 decode failed, trying latin1 encoding...")
            with open(filepath, 'r', encoding='latin1') as f:
                lines = f.readlines()

        for line in lines:
            line = line.strip()
            if not line: continue

            # Extract username from URL (e.g., https://www.tiktok.com/@username)
            match = re.search(r'https://www\.tiktok\.com/@([^/?\s]+)', line)
            if match:
                usernames.add(match.group(1))
            # Assume it's a username if not a URL and contains no typical path chars
            elif not line.startswith('http') and not ' ' in line and line:
                 if ':' not in line and '\\' not in line and '/' not in line:
                    usernames.add(line)

    except FileNotFoundError:
        print(f"üö® Error: Profile list file not found at '{filepath}'")
        return []
    except Exception as e:
        print(f"üö® Error reading profile list file: {e}")
        return []

    unique_usernames = [u for u in list(usernames) if u] # Filter out empty strings
    print(f"   Found {len(unique_usernames)} unique profile usernames.")
    return unique_usernames

# --- Helper Function: Get Keywords for Filtering ---
def get_filter_keywords(query, min_length=3):
    """ Extracts keywords from the user query for filtering results. """
    query_cleaned = query.lower().translate(str.maketrans('', '', string.punctuation))
    words = query_cleaned.split()
    # Define common English stop words (can be expanded)
    stop_words = set(["i", "am", "very", "everyday", "how", "do", "this", "in", "my", "to", "is", "a", "the", "and", "or", "it", "for", "on", "with", "as", "be", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these", "those", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", "at", "by", "for", "with", "about", "against", "between", "into", "through", "during", "before", "after", "above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again", "further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very", "s", "t", "can", "will", "just", "don", "should", "now"])
    keywords = [word for word in words if word not in stop_words and len(word) >= min_length]
    unique_keywords = list(dict.fromkeys(keywords)) # Keep order but unique
    print(f"Keywords extracted for filtering: {unique_keywords}")
    return unique_keywords

# --- Helper Function: Wait for Actor Run to Finish ---
def wait_for_run_completion(client, run_info, actor_name, max_wait_seconds):
    """ Polls the Apify platform until an actor run finishes or times out. """
    run_id = run_info.get('id')
    if not run_id: print(f"‚ùå Error: Could not get run ID for {actor_name}.") ; return None
    print(f"   Waiting for {actor_name} run ({run_id}) to complete...")
    start_time = time.time() ; last_status = None ; final_run_info = None

    with tqdm(total=max_wait_seconds // POLL_INTERVAL_SECONDS, desc=f"Waiting for {actor_name} ({run_id})", unit=" checks", leave=True) as pbar:
        while True:
            elapsed_seconds = time.time() - start_time
            if elapsed_seconds >= max_wait_seconds:
                pbar.set_description(f"TIMEOUT waiting for {actor_name} ({run_id})")
                tqdm.write(f"\n‚åõÔ∏è Warning: Run ({run_id}) did not complete within {max_wait_seconds / 60:.1f} minutes.")
                final_run_info = None
                break

            try:
                current_run_info = client.run(run_id).get()
                status = current_run_info.get('status')

                if status != last_status:
                     print(" " * 50, end='\r') # Clear previous line
                     tqdm.write(f"   [{time.strftime('%H:%M:%S')}] Run status: {status}")
                     last_status = status
                     pbar.set_description(f"Waiting for {actor_name} ({run_id}) - Status: {status}")

                if status == 'SUCCEEDED':
                    print(" " * 50, end='\r')
                    pbar.set_description(f"Run {run_id} SUCCEEDED")
                    tqdm.write(f"\n‚úÖ Run ({run_id}) finished successfully.")
                    final_run_info = current_run_info
                    pbar.update(pbar.total - pbar.n) # Complete the bar
                    break
                elif status in ['FAILED', 'ABORTED', 'TIMED_OUT']:
                    print(" " * 50, end='\r')
                    pbar.set_description(f"Run {run_id} {status}")
                    tqdm.write(f"\n‚ùå Run ({run_id}) finished with status: {status}.")
                    final_run_info = None
                    pbar.update(pbar.total - pbar.n) # Complete the bar
                    break

            except Exception as e:
                print(" " * 50, end='\r')
                tqdm.write(f"\n‚ùå Error while checking run status for {run_id}: {e}")
                # Continue polling despite error

            # Countdown between checks if not finished
            if status not in ['SUCCEEDED', 'FAILED', 'ABORTED', 'TIMED_OUT']:
                pbar.update(1)
                for i in range(POLL_INTERVAL_SECONDS, 0, -1):
                    if time.time() - start_time >= max_wait_seconds: break
                    print(f"   Next check in {i}s...          ", end='\r')
                    time.sleep(1)
                print(" " * 50, end='\r') # Clear countdown
                if time.time() - start_time >= max_wait_seconds: continue # Let main loop handle timeout

    return final_run_info

# --- Helper Function: Scrape Videos from Profiles ---
def scrape_videos_from_profiles(client, profile_usernames, actor_settings):
    """ Runs the video scraper actor for given profile usernames. """
    if not profile_usernames:
        print("No profile usernames provided, skipping video scraping.")
        return []

    print(f"\n‚ñ∂Ô∏è Starting TikTok scrape using Actor OtzYfK1ndEGdwWFKQ for {len(profile_usernames)} profile(s)")
    actor_id = "OtzYfK1ndEGdwWFKQ" # VIDEO/PROFILE ACTOR ID
    run_input = { "profiles": profile_usernames, **actor_settings }

    try:
        initial_run_info = client.actor(actor_id).call(run_input=run_input)
        print(f"‚úÖ Scraper actor run initiated. Actor ID: {actor_id}, Run ID: {initial_run_info.get('id')}")
        print(f"üíæ Dataset ID: {initial_run_info.get('defaultDatasetId')}")
        print(f"   Check full status/log: https://console.apify.com/actors/runs/{initial_run_info.get('id')}")

        final_run_info = wait_for_run_completion(
            client, initial_run_info, f"Actor {actor_id}", MAX_WAIT_MINUTES * 60
        )

        if final_run_info and final_run_info.get('status') == 'SUCCEEDED':
            dataset_id = final_run_info.get("defaultDatasetId")
            print(f"   Fetching results from dataset {dataset_id}...")
            item_count = client.dataset(dataset_id).get().get('itemCount') if dataset_id else None

            iterator = client.dataset(dataset_id).iterate_items() if dataset_id else iter([])
            desc = "Fetching results"
            if item_count: print(f"   Expected item count: {item_count}") ; desc += f" (~{item_count} items)"
            else: print(f"   Fetching items (count unknown)...")

            all_items = list(tqdm(iterator, total=item_count, desc=desc, unit=" items", leave=False))

            print(f"\n   Found {len(all_items)} raw items.")
            return all_items
        else:
            print(f"   Video scraper run did not succeed. Skipping result fetching.")
            return []

    except Exception as e:
        print(f"‚ùå Error running/monitoring video scraper actor '{actor_id}': {e}")
        return []

# --- Helper Function: Process Raw Video Data ---
def process_video_results(raw_results):
    """ Extracts specific fields from raw video results. """
    processed_data = []
    if not raw_results: return processed_data
    print(f"\n‚öôÔ∏è Processing {len(raw_results)} raw items (filtering for videos)...")

    video_count = 0
    for item in tqdm(raw_results, desc="Processing video items", unit=" item", leave=False):
        video_id = item.get('id')
        video_link = item.get('webVideoUrl') or item.get('videoUrl') or item.get('link')
        if not video_id and video_link:
             try:
                 match = re.search(r'/video/(\d+)', video_link)
                 if match: video_id = match.group(1)
             except Exception: video_id = None

        # Skip non-video items (e.g., profile info mixed in results)
        if (not video_id and not video_link) or 'followers' in item or 'following' in item:
             continue

        video_count += 1
        author_info = item.get('author', {}) or item.get('authorMeta', {})
        author_name = author_info.get('name') or author_info.get('nickname')
        author_nickname = author_info.get('uniqueId') or author_info.get('nickName')
        profile_url_vid = f"https://www.tiktok.com/@{author_nickname}" if author_nickname else None

        # Extract Subtitle Link
        subtitles_link = None ; first_other_link = None
        video_meta = item.get('videoMeta', {})
        subtitle_links = video_meta.get('subtitleLinks', []) if video_meta else []
        if isinstance(subtitle_links, list):
            for link_info in subtitle_links:
                if isinstance(link_info, dict):
                    lang = link_info.get('language') ; d_link = link_info.get('downloadLink')
                    if lang == 'eng-US': subtitles_link = d_link ; break
                    if first_other_link is None and d_link: first_other_link = d_link
            if subtitles_link is None: subtitles_link = first_other_link

        author_bio = author_info.get('signature')
        video_description = item.get('text') or item.get('desc')

        processed_item = {
            'Vid': video_id, 'Name': author_name, 'NickName': author_nickname,
            'ProfileUrl': profile_url_vid, 'TikTok_Transcription': subtitles_link,
            'TikTok_Link': video_link, 'Author Bio': author_bio,
            'Video Description': video_description
        }
        processed_data.append(processed_item)

    print(f"\n   Finished processing. Extracted {video_count} video items.")
    return processed_data

# --- Helper Function: Run Comment Scraper ---
def scrape_comments_for_videos(client, video_urls, comments_limit, replies_limit):
    """ Runs the comment scraper actor for given video URLs. """
    if not video_urls:
        print("\nNo video URLs provided, skipping comment scraping.")
        return []
    print(f"\nüí¨ Starting TikTok comment scrape for {len(video_urls)} videos (limit: {comments_limit}/post)")
    actor_id = "clockworks/tiktok-comments-scraper" # COMMENT ACTOR ID
    run_input = {
        "postURLs": video_urls,
        "commentsPerPost": comments_limit,
        "maxRepliesPerComment": replies_limit,
        "resultsPerPage": 100, # Default, can be adjusted if needed
        }
    try:
        initial_run_info = client.actor(actor_id).call(run_input=run_input)
        print(f"‚úÖ Comment scraper actor run initiated. Actor ID: {actor_id}, Run ID: {initial_run_info.get('id')}")
        print(f"üíæ Comment dataset ID: {initial_run_info.get('defaultDatasetId')}")
        print(f"   Check full status/log: https://console.apify.com/actors/runs/{initial_run_info.get('id')}")

        final_run_info = wait_for_run_completion(
            client, initial_run_info, f"Actor {actor_id}", MAX_WAIT_MINUTES * 60
            )

        if final_run_info and final_run_info.get('status') == 'SUCCEEDED':
            dataset_id = final_run_info.get("defaultDatasetId")
            print(f"   Fetching comment results from dataset {dataset_id}...")
            item_count = client.dataset(dataset_id).get().get('itemCount') if dataset_id else None

            iterator = client.dataset(dataset_id).iterate_items() if dataset_id else iter([])
            desc = "Fetching comments"
            if item_count: print(f"   Expected item count: {item_count}") ; desc += f" (~{item_count} items)"
            else: print(f"   Fetching comments (count unknown)...")

            all_comment_items = list(tqdm(iterator, total=item_count, desc=desc, unit=" comments", leave=False))

            print(f"\n   Found {len(all_comment_items)} raw comment items in total.")
            return all_comment_items
        else:
            print(f"   Comment scraper run did not succeed. Skipping result fetching.")
            return []
    except Exception as e:
        print(f"‚ùå Error running/monitoring comment scraper actor '{actor_id}': {e}")
        return []

# --- *** CORRECTED Helper Function: Process Raw Comment Data *** ---
def process_comment_results(raw_results):
    """ Extracts specific fields from raw comment results based on observed JSON structure. """
    processed_data = []
    if not raw_results: return processed_data
    print(f"\n‚öôÔ∏è Processing {len(raw_results)} raw comment items...")

    # Optional: Uncomment to print the first raw item for debugging
    # if raw_results: print("--- Sample Raw Comment Item ---"); pprint(raw_results[0]); print("---")

    for item in tqdm(raw_results, desc="Processing comments", unit=" comment", leave=False):
        author_info = item.get('author', {})
        unique_id_cmt = author_info.get('uniqueId') # Username of commenter
        video_url_cmt = item.get('videoWebUrl') # <-- Use 'videoWebUrl' based on JSON

        # Extract Vid from videoWebUrl using regex
        vid = None
        if video_url_cmt: # Only attempt if the URL exists
            try:
                match = re.search(r'/video/(\d+)', video_url_cmt)
                if match:
                    vid = match.group(1)
            except Exception as e:
                print(f"Warning: Could not extract Vid from URL '{video_url_cmt}': {e}")
                vid = None # Ensure vid is None if extraction fails

        processed_item = {
            'Vid': vid,                             # Extracted from videoWebUrl
            'Cid': item.get('cid'),                 # Comment ID
            'RepliesToId': item.get('repliesToId'), # <-- Use 'repliesToId' based on JSON
            'Comments': item.get('text'),           # The actual comment text
            'uniqueId': unique_id_cmt,              # Username of the person who commented
            'videoWebUrl': video_url_cmt            # <-- Use 'videoWebUrl' based on JSON
        }
        processed_data.append(processed_item)
    print(f"\n   Finished processing comment items.")
    return processed_data
# --- *** End CORRECTED Function *** ---


# --- Main Execution ---
if __name__ == "__main__":
    print("--- TikTok Data Extraction from Profile List ---")
    video_csv_path = None
    comment_csv_path = None

    # --- Setup Checks ---
    if not APIFY_API_TOKEN or APIFY_API_TOKEN.startswith('apify_api_') == False or len(APIFY_API_TOKEN) < 20: # Basic token format check
        print("üö® Error: Apify API token seems missing, invalid, or is a placeholder.")
        # Consider exiting if token is definitely invalid: exit()
    else:
         print("Apify API token found.") # Confirmation

    if not OUTPUT_DIRECTORY: print("üö® Error: Output directory is not set."); exit()
    try:
        os.makedirs(OUTPUT_DIRECTORY, exist_ok=True); print(f"Output directory set to: {OUTPUT_DIRECTORY}")
    except OSError as e: print(f"üö® Error creating output directory '{OUTPUT_DIRECTORY}': {e}"); exit()
    try:
        apify_client = ApifyClient(APIFY_API_TOKEN); print("Apify client initialized.")
    except Exception as e: print(f"üö® Error initializing Apify client: {e}"); exit()

    # 1. Load Profile Usernames
    profile_usernames_to_scrape = load_profile_usernames(PROFILE_LIST_FILE)
    if not profile_usernames_to_scrape: print("üö® No profile usernames loaded. Exiting."); exit()

    # 2. Scrape Videos
    raw_scraped_results = scrape_videos_from_profiles(
        apify_client, profile_usernames_to_scrape, VIDEO_ACTOR_INPUT_SETTINGS
    )

    # 3. Process Video Results
    processed_video_data = process_video_results(raw_scraped_results)

    # 4. Get Keywords for Filtering (optional)
    filter_keywords = get_filter_keywords(user_input_query)

    # 5. Filter Videos (optional)
    filtered_video_data = []
    if processed_video_data and filter_keywords:
        print(f"\nüîç Filtering {len(processed_video_data)} videos based on description/bio for keywords: {filter_keywords}...")
        for video in tqdm(processed_video_data, desc="Filtering videos", unit=" video", leave=False):
            text_to_search = f"{video.get('Video Description', '')} {video.get('Author Bio', '')}".lower()
            if any(keyword in text_to_search for keyword in filter_keywords):
                filtered_video_data.append(video)
        print(f"\n   Found {len(filtered_video_data)} videos matching query keywords in description/bio.")
    elif not filter_keywords:
         print("\n‚ö†Ô∏è No keywords extracted from query for filtering. Using all scraped videos for comment scraping.")
         filtered_video_data = processed_video_data # Use all if no keywords
    else:
        print("\nNo processed videos to filter.")

    # --- Comment Scraping Steps ---
    # 6. Extract Video URLs for Comment Scraping (from filtered or all videos)
    video_urls_for_comments = []
    if filtered_video_data:
        video_urls_for_comments = [video['TikTok_Link'] for video in filtered_video_data if video.get('TikTok_Link')]
        video_urls_for_comments = list(dict.fromkeys(video_urls_for_comments)) # Ensure unique
        print(f"\nExtracted {len(video_urls_for_comments)} unique video URLs from potentially filtered data for comment scraping.")
    else:
        print("\nNo video data available to extract URLs from for comment scraping.")

    # 7. Scrape Comments (using the extracted video URLs)
    raw_comment_results = []
    comments_limit_reached = False # Flag
    if video_urls_for_comments:
        print("\nAttempting comment scraping (may fail due to usage limits)...")
        raw_comment_results = scrape_comments_for_videos(
            apify_client, video_urls_for_comments, COMMENTS_PER_POST_LIMIT, MAX_REPLIES_PER_COMMENT
        )
        if not raw_comment_results:
             print("\nComment scraping did not return results (possibly due to limits or other errors).")
             comments_limit_reached = True
    else:
        print("\nSkipping comment scraping as no video URLs were available.")

    # 8. Process Comment Results (using the CORRECTED function)
    processed_comment_data = process_comment_results(raw_comment_results)
    # --- End Comment Scraping Steps ---

    # 9. Display Summary
    print("\n--- Final Summary ---")
    print(f"Profiles Scraped (Attempted): {len(profile_usernames_to_scrape)}")
    print(f"Raw Items Found by Video Actor: {len(raw_scraped_results)}")
    print(f"Processed Videos Extracted (Before Filter): {len(processed_video_data)}")
    print(f"Query Used for Filtering: '{user_input_query}'")
    print(f"Keywords Used for Filtering: {filter_keywords}")
    print(f"Videos After Filtering (Description/Bio): {len(filtered_video_data)}") # Videos used for comment scrape input
    print(f"Raw Comments Found by Comment Actor: {len(raw_comment_results)}")
    print(f"Processed Comments: {len(processed_comment_data)}")
    if comments_limit_reached:
         print("\n*** NOTE: Comment scraping may have been skipped or failed, potentially due to usage limits. ***")

    # --- 10. SAVING PROCESSED DATA TO CSV ---
    # Save Processed Video Results
    if processed_video_data:
        print("\nüíæ Saving ALL PROCESSED video results to CSV...")
        try:
            video_csv_path = os.path.join(OUTPUT_DIRECTORY, 'tiktok_videos_processed_all.csv')
            temp_df_vid = pd.DataFrame(processed_video_data)
            # Ensure all columns exist, add if missing
            for col in VIDEO_COLUMNS:
                if col not in temp_df_vid.columns: temp_df_vid[col] = None
            videos_df_to_save = temp_df_vid[VIDEO_COLUMNS] # Select/order columns
            videos_df_to_save.to_csv(video_csv_path, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_MINIMAL)
            print(f"   All processed video results saved to: {video_csv_path}")
        except Exception as e:
            print(f"‚ùå Error saving processed video results to CSV: {e}")
            video_csv_path = None
    else:
        print("\nNo processed video results to save.")
        video_csv_path = None

    # Save Processed Comment Results
    if processed_comment_data:
        print("\nüíæ Saving PROCESSED comment results to CSV...")
        try:
             comment_csv_path = os.path.join(OUTPUT_DIRECTORY, 'tiktok_comments_processed.csv')
             temp_df_com = pd.DataFrame(processed_comment_data)
             # Ensure all columns exist, add if missing
             for col in COMMENT_COLUMNS:
                 if col not in temp_df_com.columns: temp_df_com[col] = None
             comments_df_to_save = temp_df_com[COMMENT_COLUMNS] # Select/order columns
             comments_df_to_save.to_csv(comment_csv_path, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_MINIMAL)
             print(f"   Processed comment results saved to: {comment_csv_path}")
        except Exception as e:
            print(f"‚ùå Error saving processed comment results to CSV: {e}")
            comment_csv_path = None
    else:
        print("\nNo processed comment results to save.")
        comment_csv_path = None

    print("\n--- Program Finished ---")

    # --- Analyze Saved Video Data for Missing Transcriptions ---
    print("\n--- Analyzing Saved Video Data for Missing Transcriptions ---")
    if video_csv_path and os.path.exists(video_csv_path):
        try:
            df_saved_videos = pd.read_csv(video_csv_path, encoding='utf-8-sig')
            print(f"Successfully re-read: {video_csv_path}")
            transcription_col_name = 'TikTok_Transcription'
            if transcription_col_name in df_saved_videos.columns:
                def highlight_missing(row):
                    # Check if NaN, None, or empty string after stripping
                    val = row[transcription_col_name]
                    is_missing = pd.isna(val) or str(val).strip() == ''
                    return ['background-color: yellow' if is_missing else '' for _ in row]
                styled_df = df_saved_videos.style.apply(highlight_missing, axis=1)
                print("\nDisplaying table with missing transcription links highlighted:")
                display(styled_df) # Display in Jupyter/IPython
            else:
                print(f"Column '{transcription_col_name}' not found in '{video_csv_path}'.")
        except Exception as e:
            print(f"‚ùå Error reading or styling the saved video CSV: {e}")
    elif video_csv_path: print(f"File not found: {video_csv_path}. Cannot analyze.")
    else: print("Video CSV was not saved successfully. Cannot analyze.")
    # --- End Highlighting Section ---
