#Text Normalisation

import re
import pandas as pd
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')

#Step 1: Environment Setup
!pip install pandas numpy openpyxl keybert transformers torch scikit-learn


# Loading the Merged Comment data
comments_df = pd.read_csv("G:/Other computers/My Laptop (1)/Pelajaran UiTM/Classes and Lectures (Semester 6)/CSP650/Developments/Scraps/Testing Files/Outpus/Standardized Comments Scaled Down.csv")

# Loading the Video with Transcription data
videos_df = pd.read_csv("G:/Other computers/My Laptop (1)/Pelajaran UiTM/Classes and Lectures (Semester 6)/CSP650/Developments/Scraps/Testing Files/Outpus/Video with Transcription.csv")

#Inspecting Data
comments_df.head()
videos_df.head()

# Handling Missing Values Section
# Check missing values in comments
print(comments_df.isnull().sum())

# Remove missing values or fill with placeholder text
comments_df.dropna(subset=['Comments'], inplace=True)

# Alternatively, to fill:
# comments_df['comments'].fillna("No comment provided", inplace=True)

# Check missing values in videos
print(videos_df.isnull().sum())

videos_df.dropna(subset=['TikTok_Transcription'], inplace=True)

# Removing duplicates Section
# Remove duplicates from comments
comments_df.drop_duplicates(subset=['Comments'], inplace=True)

# Remove duplicates from videos
videos_df.drop_duplicates(subset=['TikTok_Transcription'], inplace=True)

# Text Normalization & Cleaning
def clean_text(text):
    text = str(text).lower()  # convert text to lowercase
    text = re.sub(r'http\S+', '', text)  # remove URLs
    text = re.sub(r'[^\w\s]', '', text)  # remove punctuation
    text = re.sub(r'\d+', '', text)  # remove numbers
    text = re.sub(r'\s+', ' ', text).strip()  # normalize whitespace
    return text

# Apply cleaning to comments and transcriptions
comments_df['Clean_Comments'] = comments_df['Comments'].apply(clean_text)
videos_df['Clean_Transcriptions'] = videos_df['TikTok_Transcription'].apply(clean_text)
