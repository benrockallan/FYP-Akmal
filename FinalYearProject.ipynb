{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4532b79-fdb1-41bf-9013-86a10262f0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEPS 1\n",
    "#This coding aimed to create Column for VID\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('G:/Other computers/My Laptop (1)/Pelajaran UiTM/Classes and Lectures (Semester 5)/FINAL YEAR PROJECT/Developments/Collected Data V1.csv')\n",
    "\n",
    "# Assuming the columns containing TikTok links and captions exist (update column names as needed)\n",
    "tiktok_links_column = 'TikTok_Link'  # Update with the actual column name\n",
    "tiktok_captions_column = 'TikTok_Captions'  # Update with the actual column name\n",
    "\n",
    "# Extract relevant columns\n",
    "selected_columns = [tiktok_links_column, tiktok_captions_column]\n",
    "df_filtered = df[selected_columns].copy()\n",
    "\n",
    "# Create a new column 'VID' as a unique identifier (e.g., based on index)\n",
    "df_filtered['VID'] = df_filtered.index + 1\n",
    "\n",
    "# Save the cleaned dataset\n",
    "output_path = '/mnt/data/TikTok_Filtered_Data.csv'\n",
    "df_filtered.to_csv(output_path, index=False)\n",
    "\n",
    "# Display the dataframe\n",
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(name=\"Filtered TikTok Data\", dataframe=df_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e2de0f-f70e-4553-bdbd-8b6a7161ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEPS 2\n",
    "#This coding aimed to generate unique VID for each row\n",
    "\n",
    "import pandas as pd\n",
    "import uuid\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"G:/Other computers/My Laptop (1)/Pelajaran UiTM/Classes and Lectures (Semester 5)/FINAL YEAR PROJECT/Developments/Collected Data V1.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Extract relevant columns\n",
    "columns_to_extract = {\n",
    "    \"authorMeta/profileUrl\": \"TikTok_Profile_Link\",\n",
    "    \"videoMeta/subtitleLinks/1/downloadLink\": \"TikTok_Transcription\"\n",
    "}\n",
    "\n",
    "df_extracted = df[list(columns_to_extract.keys())].rename(columns=columns_to_extract)\n",
    "\n",
    "# Generate a unique VID for each row\n",
    "df_extracted[\"VID\"] = [str(uuid.uuid4()) for _ in range(len(df_extracted))]\n",
    "\n",
    "# Save the processed data\n",
    "output_file = \"G:/Other computers/My Laptop (1)/Pelajaran UiTM/Classes and Lectures (Semester 5)/FINAL YEAR PROJECT/Developments/TikTok_Processed_Data.csv\"\n",
    "df_extracted.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Processed file saved successfully:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e148d8a4-12b9-4119-8475-6c98bcde1fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File successfully copied to: G:\\Other computers\\My Laptop (1)\\Pelajaran UiTM\\Classes and Lectures (Semester 5)\\FINAL YEAR PROJECT\\Developments\\Downloads\\sorted_transcriptionsV2.xlsx\n"
     ]
    }
   ],
   "source": [
    "#STEPS 3\n",
    "#This coding aimed to sort the timestamp\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = r\"G:\\Other computers\\My Laptop (1)\\Pelajaran UiTM\\Classes and Lectures (Semester 5)\\FINAL YEAR PROJECT\\Developments\\sorted_transcriptions V1.xlsx\"\n",
    "xls = pd.ExcelFile(file_path)\n",
    "\n",
    "# Load the first sheet\n",
    "df = xls.parse('Sheet1')\n",
    "\n",
    "# Function to extract the start time from the transcription column\n",
    "def extract_start_time(transcription):\n",
    "    match = re.match(r\"(\\d{2}:\\d{2}:\\d{2}\\.\\d{3})\", str(transcription))  # Convert to str in case of NaN values\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Apply extraction function\n",
    "df[\"Start_Timestamp\"] = df[\"TikTok_Transcription\"].apply(extract_start_time)\n",
    "\n",
    "# Convert to datetime for sorting\n",
    "df[\"Start_Timestamp\"] = pd.to_datetime(df[\"Start_Timestamp\"], format=\"%H:%M:%S.%f\", errors='coerce')\n",
    "\n",
    "# Sort the dataframe by timestamp\n",
    "df_sorted = df.sort_values(by=\"Start_Timestamp\")\n",
    "\n",
    "# Define path for sorted output\n",
    "sorted_file_path = r\"G:\\Other computers\\My Laptop (1)\\Pelajaran UiTM\\Classes and Lectures (Semester 5)\\FINAL YEAR PROJECT\\Developments\\sorted_transcriptionsV2.xlsx\"\n",
    "\n",
    "# Save sorted data to a new Excel file\n",
    "df_sorted.to_excel(sorted_file_path, index=False)\n",
    "\n",
    "# Ensure file is saved before copying\n",
    "if os.path.exists(sorted_file_path):\n",
    "    # Define the path to copy the file\n",
    "    download_folder = r\"G:\\Other computers\\My Laptop (1)\\Pelajaran UiTM\\Classes and Lectures (Semester 5)\\FINAL YEAR PROJECT\\Developments\\Downloads\"\n",
    "    \n",
    "    # Ensure the destination folder exists\n",
    "    os.makedirs(download_folder, exist_ok=True)\n",
    "    \n",
    "    # Copy the file to the destination\n",
    "    new_download_path = os.path.join(download_folder, \"sorted_transcriptionsV2.xlsx\")\n",
    "    shutil.copy(sorted_file_path, new_download_path)\n",
    "    \n",
    "    print(f\"File successfully copied to: {new_download_path}\")\n",
    "else:\n",
    "    print(\"Error: Sorted file was not created successfully. Check file permissions and paths.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf39d58a-6ffb-48ac-bcc2-30b6cfdb4813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped 178 unique comment IDs to simple sequential numbers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "file_path = r\"G:\\Other computers\\My Laptop (1)\\Pelajaran UiTM\\Classes and Lectures (Semester 5)\\FINAL YEAR PROJECT\\Developments\\Comments for Vid 1.csv\"\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Create a mapping from original cid to simplified cid\n",
    "unique_cids = df['cid'].unique()\n",
    "cid_mapping = {original_cid: simple_cid for simple_cid, original_cid in enumerate(unique_cids, 1)}\n",
    "\n",
    "# Apply the mapping to cid column\n",
    "df['simplified_cid'] = df['cid'].map(cid_mapping)\n",
    "\n",
    "# Apply the same mapping to repliesToId column (only for non-NaN values)\n",
    "# First create a mask for non-NaN values\n",
    "non_nan_mask = ~df['repliesToId'].isna()\n",
    "\n",
    "# Apply mapping only to non-NaN values\n",
    "df.loc[non_nan_mask, 'simplified_repliesToId'] = df.loc[non_nan_mask, 'repliesToId'].map(cid_mapping)\n",
    "\n",
    "# NaN values remain NaN\n",
    "df.loc[~non_nan_mask, 'simplified_repliesToId'] = np.nan\n",
    "\n",
    "# You can keep both original and simplified IDs or replace the originals\n",
    "# Option 1: Keep both original and simplified\n",
    "df_with_both = df.copy()\n",
    "\n",
    "# Option 2: Replace originals with simplified versions\n",
    "df_simplified = df.copy()\n",
    "df_simplified['cid'] = df_simplified['simplified_cid']\n",
    "df_simplified['repliesToId'] = df_simplified['simplified_repliesToId']\n",
    "df_simplified = df_simplified.drop(['simplified_cid', 'simplified_repliesToId'], axis=1)\n",
    "\n",
    "# Save the results\n",
    "df_with_both.to_csv('Comments_with_simplified_ids.csv', index=False)\n",
    "df_simplified.to_csv('Comments_simplified.csv', index=False)\n",
    "\n",
    "print(f\"Mapped {len(unique_cids)} unique comment IDs to simple sequential numbers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5e76f58-1ea9-4ad7-9837-55ed1acfc9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped 108 unique comment IDs to simple sequential numbers.\n",
      "Files saved to:\n",
      "1. G:\\Other computers\\My Laptop (1)\\Pelajaran UiTM\\Classes and Lectures (Semester 5)\\FINAL YEAR PROJECT\\Developments\\Simplified Comments\\Comments_with_simplified_ids.csv\n",
      "2. G:\\Other computers\\My Laptop (1)\\Pelajaran UiTM\\Classes and Lectures (Semester 5)\\FINAL YEAR PROJECT\\Developments\\Simplified Comments\\Comments_simplified.csv\n"
     ]
    }
   ],
   "source": [
    "#This coding aimed to map CID to simplified CID and RepliesToID to simplified version\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the destination directory\n",
    "destination_dir = r\"G:\\Other computers\\My Laptop (1)\\Pelajaran UiTM\\Classes and Lectures (Semester 5)\\FINAL YEAR PROJECT\\Developments\\Simplified Comments\"\n",
    "\n",
    "file_path = r\"G:\\Other computers\\My Laptop (1)\\Pelajaran UiTM\\Classes and Lectures (Semester 5)\\FINAL YEAR PROJECT\\Developments\\Comments\\Comments for Vid 2.csv\"\n",
    "\n",
    "# Make sure the directory exists\n",
    "if not os.path.exists(destination_dir):\n",
    "    os.makedirs(destination_dir)\n",
    "    print(f\"Created directory: {destination_dir}\")\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Create a mapping from original cid to simplified cid\n",
    "unique_cids = df['cid'].unique()\n",
    "cid_mapping = {original_cid: simple_cid for simple_cid, original_cid in enumerate(unique_cids, 1)}\n",
    "\n",
    "# Apply the mapping to cid column\n",
    "df['simplified_cid'] = df['cid'].map(cid_mapping)\n",
    "\n",
    "# Apply the same mapping to repliesToId column (only for non-NaN values)\n",
    "# First create a mask for non-NaN values\n",
    "non_nan_mask = ~df['repliesToId'].isna()\n",
    "\n",
    "# Apply mapping only to non-NaN values\n",
    "df.loc[non_nan_mask, 'simplified_repliesToId'] = df.loc[non_nan_mask, 'repliesToId'].map(cid_mapping)\n",
    "\n",
    "# NaN values remain NaN\n",
    "df.loc[~non_nan_mask, 'simplified_repliesToId'] = np.nan\n",
    "\n",
    "# You can keep both original and simplified IDs or replace the originals\n",
    "# Option 1: Keep both original and simplified\n",
    "df_with_both = df.copy()\n",
    "\n",
    "# Option 2: Replace originals with simplified versions\n",
    "df_simplified = df.copy()\n",
    "df_simplified['cid'] = df_simplified['simplified_cid']\n",
    "df_simplified['repliesToId'] = df_simplified['simplified_repliesToId']\n",
    "df_simplified = df_simplified.drop(['simplified_cid', 'simplified_repliesToId'], axis=1)\n",
    "\n",
    "# Save the results to the specified directory\n",
    "both_file_path = os.path.join(destination_dir, 'Comments_with_simplified_ids.csv')\n",
    "simplified_file_path = os.path.join(destination_dir, 'Comments_simplified.csv')\n",
    "\n",
    "df_with_both.to_csv(both_file_path, index=False)\n",
    "df_simplified.to_csv(simplified_file_path, index=False)\n",
    "\n",
    "print(f\"Mapped {len(unique_cids)} unique comment IDs to simple sequential numbers.\")\n",
    "print(f\"Files saved to:\")\n",
    "print(f\"1. {both_file_path}\")\n",
    "print(f\"2. {simplified_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30393647-64b9-4317-9802-c91d204393c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped 321 unique comment IDs to simple sequential numbers.\n",
      "File saved to: G:\\Other computers\\My Laptop (1)\\Pelajaran UiTM\\Classes and Lectures (Semester 5)\\FINAL YEAR PROJECT\\Developments\\Simplified Comments\\Comments_with_simplified_ids 4 and 5.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the destination directory\n",
    "destination_dir = r\"G:\\Other computers\\My Laptop (1)\\Pelajaran UiTM\\Classes and Lectures (Semester 5)\\FINAL YEAR PROJECT\\Developments\\Simplified Comments\"\n",
    "\n",
    "file_path = r\"G:\\Other computers\\My Laptop (1)\\Pelajaran UiTM\\Classes and Lectures (Semester 5)\\FINAL YEAR PROJECT\\Developments\\Comments\\Comments for Vid 5.csv\"\n",
    "\n",
    "# Ensure the destination directory exists\n",
    "if not os.path.exists(destination_dir):\n",
    "    os.makedirs(destination_dir)\n",
    "    print(f\"Created directory: {destination_dir}\")\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop the specified columns\n",
    "columns_to_drop = ['avatarThumbnail', 'createTime', 'createTimeISO', 'diggCount', 'input', 'uid' , 'replyCommentTotal', 'submittedVideoUrl']\n",
    "df = df.drop(columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')\n",
    "\n",
    "# Create a mapping from original cid to simplified cid\n",
    "unique_cids = df['cid'].unique()\n",
    "cid_mapping = {original_cid: simple_cid for simple_cid, original_cid in enumerate(unique_cids, 1)}\n",
    "\n",
    "# Apply the mapping to cid column\n",
    "df['simplified_cid'] = df['cid'].map(cid_mapping)\n",
    "\n",
    "# Apply the same mapping to repliesToId column (only for non-NaN values)\n",
    "non_nan_mask = ~df['repliesToId'].isna()\n",
    "df.loc[non_nan_mask, 'simplified_repliesToId'] = df.loc[non_nan_mask, 'repliesToId'].map(cid_mapping)\n",
    "\n",
    "# Keep NaN values unchanged\n",
    "df.loc[~non_nan_mask, 'simplified_repliesToId'] = np.nan\n",
    "\n",
    "# Keep both original and simplified IDs\n",
    "df_with_both = df.copy()\n",
    "\n",
    "# Save only `Comments_with_simplified_ids.csv`\n",
    "both_file_path = os.path.join(destination_dir, 'Comments_with_simplified_ids 4 and 5.csv')\n",
    "df_with_both.to_csv(both_file_path, index=False)\n",
    "\n",
    "print(f\"Mapped {len(unique_cids)} unique comment IDs to simple sequential numbers.\")\n",
    "print(f\"File saved to: {both_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b40fdc9-358a-48f3-8497-4e9f60a0bf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped 135 unique comment IDs to simple sequential numbers.\n",
      "File saved to: G:\\Other computers\\My Laptop (1)\\Pelajaran UiTM\\Classes and Lectures (Semester 5)\\FINAL YEAR PROJECT\\Developments\\Simplified Comments\\Comments_with_simplified_ids 5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_40848\\2534266256.py:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['simplified_repliesToId'].fillna(np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#NOT USEFUL NOW/Depreciated\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the destination directory\n",
    "destination_dir = r\"G:\\Other computers\\My Laptop (1)\\Pelajaran UiTM\\Classes and Lectures (Semester 5)\\FINAL YEAR PROJECT\\Developments\\Simplified Comments\"\n",
    "\n",
    "file_path = r\"G:\\Other computers\\My Laptop (1)\\Pelajaran UiTM\\Classes and Lectures (Semester 5)\\FINAL YEAR PROJECT\\Developments\\Comments\\Comments for Vid 5.csv\"\n",
    "\n",
    "# Make sure the directory exists\n",
    "if not os.path.exists(destination_dir):\n",
    "    os.makedirs(destination_dir)\n",
    "    print(f\"Created directory: {destination_dir}\")\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Create a mapping from original cid to simplified cid\n",
    "unique_cids = df['cid'].unique()\n",
    "cid_mapping = {original_cid: simple_cid for simple_cid, original_cid in enumerate(unique_cids, 1)}\n",
    "\n",
    "# Apply the mapping to cid column\n",
    "df['simplified_cid'] = df['cid'].map(cid_mapping)\n",
    "\n",
    "# Apply the same mapping to repliesToId column (only for non-NaN values)\n",
    "df['simplified_repliesToId'] = df['repliesToId'].map(cid_mapping)\n",
    "df['simplified_repliesToId'].fillna(np.nan, inplace=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['avatarThumbnail', 'createTime', 'createTimeISO', 'diggCount', 'input', 'replyCommentTotal', 'submittedVideoUrl']\n",
    "df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)\n",
    "\n",
    "# Replace original IDs with simplified ones\n",
    "df['cid'] = df['simplified_cid']\n",
    "df['repliesToId'] = df['simplified_repliesToId']\n",
    "\n",
    "# Drop temporary simplified columns\n",
    "df.drop(['simplified_cid', 'simplified_repliesToId'], axis=1, inplace=True)\n",
    "\n",
    "# Add new column 'Vid' with all values as 5\n",
    "df['Vid'] = 5\n",
    "\n",
    "# Save the results\n",
    "output_file_path = os.path.join(destination_dir, 'Comments_with_simplified_ids 7.csv')\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Mapped {len(unique_cids)} unique comment IDs to simple sequential numbers.\")\n",
    "print(f\"File saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94735bef-91e4-47a0-9399-25705c47ea50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped 200 unique comment IDs to simple sequential numbers.\n",
      "File saved to: G:\\Other computers\\My Laptop (1)\\Pelajaran UiTM\\Classes and Lectures (Semester 5)\\FINAL YEAR PROJECT\\Developments\\Simplified Data\\Comments For Vid 12.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_13204\\655010279.py:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['simplified_repliesToId'].fillna(np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#STEPS 4\n",
    "#Cid and RepliestoId Changer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the destination directory\n",
    "destination_dir = r\"G:\\Other computers\\My Laptop (1)\\Pelajaran UiTM\\Classes and Lectures (Semester 5)\\FINAL YEAR PROJECT\\Developments\\Simplified Data\"\n",
    "\n",
    "file_path = r\"G:\\Other computers\\My Laptop (1)\\Pelajaran UiTM\\Classes and Lectures (Semester 5)\\FINAL YEAR PROJECT\\Developments\\Comments\\Comments for Vid 12 Test.csv\"\n",
    "\n",
    "# Make sure the directory exists\n",
    "if not os.path.exists(destination_dir):\n",
    "    os.makedirs(destination_dir)\n",
    "    print(f\"Created directory: {destination_dir}\")\n",
    "\n",
    "# Load the CSV file with proper encoding\n",
    "df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "\n",
    "# Create a mapping from original cid to simplified cid\n",
    "unique_cids = df['cid'].unique()\n",
    "cid_mapping = {original_cid: simple_cid for simple_cid, original_cid in enumerate(unique_cids, 1)}\n",
    "\n",
    "# Apply the mapping to cid column\n",
    "df['simplified_cid'] = df['cid'].map(cid_mapping)\n",
    "\n",
    "# Apply the same mapping to repliesToId column (only for non-NaN values)\n",
    "df['simplified_repliesToId'] = df['repliesToId'].map(cid_mapping)\n",
    "df['simplified_repliesToId'].fillna(np.nan, inplace=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['avatarThumbnail', 'createTime', 'createTimeISO', 'diggCount', 'input', 'replyCommentTotal', 'submittedVideoUrl']\n",
    "df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)\n",
    "\n",
    "# Replace original IDs with simplified ones\n",
    "df['cid'] = df['simplified_cid']\n",
    "df['repliesToId'] = df['simplified_repliesToId']\n",
    "\n",
    "# Drop temporary simplified columns\n",
    "df.drop(['simplified_cid', 'simplified_repliesToId'], axis=1, inplace=True)\n",
    "\n",
    "# Add new column 'Vid' with all values as 5\n",
    "df['Vid'] = 12\n",
    "\n",
    "# Save the results with proper encoding to keep emojis intact\n",
    "output_file_path = os.path.join(destination_dir, 'Comments For Vid 12.csv')\n",
    "df.to_csv(output_file_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Mapped {len(unique_cids)} unique comment IDs to simple sequential numbers.\")\n",
    "print(f\"File saved to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "542be903-44cb-4486-82f5-21dda3196419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_23904\\2204650382.py:36: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['simplified_repliesToId'].fillna(np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#STEPS 5\n",
    "#Vid Column Adder \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "\n",
    "# Function to process selected files\n",
    "def process_files():\n",
    "    file_paths = filedialog.askopenfilenames(title=\"Select CSV Files\", filetypes=[(\"CSV Files\", \"*.csv\")])\n",
    "    if not file_paths:\n",
    "        messagebox.showwarning(\"No File Selected\", \"Please select at least one CSV file.\")\n",
    "        return\n",
    "    \n",
    "    destination_dir = filedialog.askdirectory(title=\"Select Destination Folder\")\n",
    "    if not destination_dir:\n",
    "        messagebox.showwarning(\"No Destination Selected\", \"Please select a destination folder.\")\n",
    "        return\n",
    "    \n",
    "    vid_value = vid_entry.get().strip()\n",
    "    if not vid_value.isdigit():\n",
    "        messagebox.showerror(\"Invalid Input\", \"Please enter a numeric value for the Vid column.\")\n",
    "        return\n",
    "    vid_value = int(vid_value)\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "            \n",
    "            unique_cids = df['cid'].unique()\n",
    "            cid_mapping = {original_cid: simple_cid for simple_cid, original_cid in enumerate(unique_cids, 1)}\n",
    "            \n",
    "            df['simplified_cid'] = df['cid'].map(cid_mapping)\n",
    "            df['simplified_repliesToId'] = df['repliesToId'].map(cid_mapping)\n",
    "            df['simplified_repliesToId'].fillna(np.nan, inplace=True)\n",
    "            \n",
    "            columns_to_drop = ['avatarThumbnail', 'createTime', 'createTimeISO', 'diggCount', 'input', 'replyCommentTotal', 'submittedVideoUrl']\n",
    "            df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)\n",
    "            \n",
    "            df['cid'] = df['simplified_cid']\n",
    "            df['repliesToId'] = df['simplified_repliesToId']\n",
    "            df.drop(['simplified_cid', 'simplified_repliesToId'], axis=1, inplace=True)\n",
    "            \n",
    "            df['Vid'] = vid_value\n",
    "            \n",
    "            base_filename = f\"Comments For Vid {vid_value}.csv\"\n",
    "            output_file_path = os.path.join(destination_dir, base_filename)\n",
    "            df.to_csv(output_file_path, index=False, encoding='utf-8-sig')\n",
    "            \n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error Processing File\", f\"An error occurred while processing {os.path.basename(file_path)}:\\n{e}\")\n",
    "    \n",
    "    messagebox.showinfo(\"Success\", \"All files have been processed and saved separately!\")\n",
    "\n",
    "# Create GUI\n",
    "root = tk.Tk()\n",
    "root.title(\"CSV Simplifier App\")\n",
    "root.geometry(\"400x250\")\n",
    "\n",
    "tk.Label(root, text=\"Enter Vid Value:\").pack(pady=5)\n",
    "vid_entry = tk.Entry(root)\n",
    "vid_entry.pack(pady=5)\n",
    "\n",
    "# User must enter Vid before proceeding\n",
    "def check_vid():\n",
    "    if not vid_entry.get().strip():\n",
    "        messagebox.showwarning(\"Missing Input\", \"Please enter a Vid value before proceeding.\")\n",
    "        return\n",
    "    process_files()\n",
    "\n",
    "tk.Button(root, text=\"Select and Process CSV Files\", command=check_vid).pack(pady=10)\n",
    "\n",
    "tk.Button(root, text=\"Exit\", command=root.destroy).pack(pady=5)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f52130cc-5a20-41e1-ad70-ed788636a084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEPS 6\n",
    "#Comment Standardizer V3 (Most Important)\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import difflib\n",
    "import re\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox, scrolledtext, ttk\n",
    "from io import StringIO\n",
    "import sys\n",
    "import chardet\n",
    "\n",
    "# Define expected columns\n",
    "EXPECTED_COLUMNS = [\"Vid\", \"Cid\", \"RepliesToId\", \"Comments\", \"uniqueId\", \"videoWebUrl\"]\n",
    "EXPECTED_COLUMNS_LOWER = {col.lower(): col for col in EXPECTED_COLUMNS}\n",
    "SPECIAL_MAPPINGS = {\n",
    "    \"text\": \"Comments\",\n",
    "    \"cid\": \"Cid\",\n",
    "    \"repliestoid\": \"RepliesToId\"\n",
    "}\n",
    "\n",
    "# Columns to specifically drop\n",
    "COLUMNS_TO_ALWAYS_DROP = [\"uid\", \"Uid\"]\n",
    "\n",
    "def match_column_name(column):\n",
    "    # First check if this is a column we always want to drop\n",
    "    if column.lower() in [col.lower() for col in COLUMNS_TO_ALWAYS_DROP]:\n",
    "        return None  # Return None to indicate this column should be dropped\n",
    "        \n",
    "    if column in EXPECTED_COLUMNS:\n",
    "        return column\n",
    "    if column.lower() in SPECIAL_MAPPINGS:\n",
    "        return SPECIAL_MAPPINGS[column.lower()]\n",
    "    if column.lower() in EXPECTED_COLUMNS_LOWER:\n",
    "        return EXPECTED_COLUMNS_LOWER[column.lower()]\n",
    "    clean_column = re.sub(r'[^a-zA-Z0-9]', '', column)\n",
    "    if clean_column.lower() in EXPECTED_COLUMNS_LOWER:\n",
    "        return EXPECTED_COLUMNS_LOWER[clean_column.lower()]\n",
    "    matches = []\n",
    "    for expected_col in EXPECTED_COLUMNS:\n",
    "        similarity = difflib.SequenceMatcher(None, column.lower(), expected_col.lower()).ratio()\n",
    "        matching_chars = sum(1 for c1, c2 in zip(column.lower(), expected_col.lower()) if c1 == c2)\n",
    "        if matching_chars >= 2:\n",
    "            match_proportion = matching_chars / max(len(column), len(expected_col))\n",
    "            score = similarity * 0.6 + match_proportion * 0.4\n",
    "            matches.append((expected_col, score, matching_chars))\n",
    "    if matches:\n",
    "        best_match = max(matches, key=lambda x: (x[1], x[2]))\n",
    "        return best_match[0]\n",
    "    return None\n",
    "\n",
    "def process_csv_file(file_path, output_dir, log_callback):\n",
    "    \"\"\"Process a single CSV file with a callback for logging output\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    log_callback(f\"\\nProcessing file: {file_path}\")\n",
    "    \n",
    "    # First, try to detect encoding\n",
    "    log_callback(\"Detecting file encoding...\")\n",
    "    with open(file_path, 'rb') as f:\n",
    "        rawdata = f.read()\n",
    "        detected = chardet.detect(rawdata)\n",
    "        detected_encoding = detected['encoding']\n",
    "        confidence = detected['confidence']\n",
    "        log_callback(f\"Detected encoding: {detected_encoding} (confidence: {confidence:.2f})\")\n",
    "    \n",
    "    # List of encodings to try, starting with the detected one\n",
    "    encodings_to_try = []\n",
    "    if detected_encoding:\n",
    "        encodings_to_try.append(detected_encoding)\n",
    "    \n",
    "    # Add other common encodings that might handle special characters\n",
    "    additional_encodings = [\n",
    "        'utf-8-sig', 'utf-8', 'latin1', 'cp1252', 'iso-8859-1', \n",
    "        'windows-1250', 'windows-1252', 'mac_roman'\n",
    "    ]\n",
    "    \n",
    "    # Add the additional encodings not already in the list\n",
    "    for enc in additional_encodings:\n",
    "        if enc.lower() != detected_encoding.lower():\n",
    "            encodings_to_try.append(enc)\n",
    "    \n",
    "    # Try reading with each encoding\n",
    "    df = None\n",
    "    successful_encoding = None\n",
    "    \n",
    "    for encoding in encodings_to_try:\n",
    "        try:\n",
    "            log_callback(f\"Trying to read with {encoding} encoding...\")\n",
    "            # Use error_bad_lines=False to skip problematic lines\n",
    "            if sys.version_info >= (3, 0):\n",
    "                # For Pandas >= 1.3.0\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path, encoding=encoding, on_bad_lines='skip')\n",
    "                except TypeError:\n",
    "                    # For older Pandas versions\n",
    "                    df = pd.read_csv(file_path, encoding=encoding, error_bad_lines=False)\n",
    "            else:\n",
    "                df = pd.read_csv(file_path, encoding=encoding, error_bad_lines=False)\n",
    "                \n",
    "            log_callback(f\"Successfully read file with {encoding} encoding\")\n",
    "            successful_encoding = encoding\n",
    "            break\n",
    "        except Exception as e:\n",
    "            log_callback(f\"Error with {encoding} encoding: {str(e)}\")\n",
    "    \n",
    "    if df is None:\n",
    "        # Last resort: try to read file with the most permissive settings\n",
    "        try:\n",
    "            log_callback(\"Attempting to read file with maximum error tolerance...\")\n",
    "            df = pd.read_csv(file_path, encoding='latin1', sep=',', quoting=3, \n",
    "                            escapechar='\\\\', on_bad_lines='skip')\n",
    "            successful_encoding = 'latin1 (with special handling)'\n",
    "            log_callback(\"Successfully read file with special handling\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to read the CSV file with any of the supported encodings: {str(e)}\")\n",
    "    \n",
    "    # Check for presence of special characters\n",
    "    try:\n",
    "        sample = df.head(5).to_string()\n",
    "        has_special_chars = any(ord(c) > 127 for c in sample)\n",
    "        if has_special_chars:\n",
    "            log_callback(\"Detected special characters or emojis in the file, will preserve them in output\")\n",
    "    except:\n",
    "        log_callback(\"Could not check for special characters\")\n",
    "    \n",
    "    original_columns = df.columns.tolist()\n",
    "    column_mapping = {}\n",
    "    columns_to_drop = []\n",
    "    \n",
    "    # Check for uid/Uid columns that need to be dropped\n",
    "    uid_columns = [col for col in original_columns if col.lower() in [c.lower() for c in COLUMNS_TO_ALWAYS_DROP]]\n",
    "    if uid_columns:\n",
    "        log_callback(f\"Found columns to specifically drop: {', '.join(uid_columns)}\")\n",
    "    \n",
    "    for col in original_columns:\n",
    "        matched_col = match_column_name(col)\n",
    "        if matched_col:\n",
    "            column_mapping[col] = matched_col\n",
    "        else:\n",
    "            columns_to_drop.append(col)\n",
    "    \n",
    "    renames_needed = any(k != v for k, v in column_mapping.items())\n",
    "    drops_needed = len(columns_to_drop) > 0\n",
    "    missing_columns = set(EXPECTED_COLUMNS) - set(column_mapping.values())\n",
    "    additions_needed = len(missing_columns) > 0\n",
    "    \n",
    "    if not (renames_needed or drops_needed or additions_needed):\n",
    "        log_callback(f\"No changes needed for {os.path.basename(file_path)}\")\n",
    "        return\n",
    "    \n",
    "    if renames_needed:\n",
    "        log_callback(\"Renaming columns:\")\n",
    "        for old, new in column_mapping.items():\n",
    "            if old != new:\n",
    "                log_callback(f\"  {old} -> {new}\")\n",
    "        df = df.rename(columns=column_mapping)\n",
    "    \n",
    "    if drops_needed:\n",
    "        log_callback(\"Dropping columns:\")\n",
    "        for col in columns_to_drop:\n",
    "            log_callback(f\"  {col}\")\n",
    "        df = df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    if additions_needed:\n",
    "        log_callback(\"Adding missing columns:\")\n",
    "        for col in missing_columns:\n",
    "            log_callback(f\"  {col}\")\n",
    "            df[col] = None\n",
    "    \n",
    "    df = df[EXPECTED_COLUMNS]\n",
    "    \n",
    "    filename = os.path.basename(file_path)\n",
    "    output_path = os.path.join(output_dir, f\"{filename}\")\n",
    "    \n",
    "    # Try to save with the successful encoding, but fall back to UTF-8 with error handling if needed\n",
    "    try:\n",
    "        log_callback(f\"Saving with {successful_encoding} encoding to preserve special characters\")\n",
    "        df.to_csv(output_path, index=False, encoding=successful_encoding)\n",
    "    except Exception as e:\n",
    "        log_callback(f\"Error saving with {successful_encoding}: {str(e)}\")\n",
    "        log_callback(\"Trying to save with utf-8-sig encoding\")\n",
    "        df.to_csv(output_path, index=False, encoding='utf-8-sig', errors='replace')\n",
    "    \n",
    "    log_callback(f\"Saved standardized file to {output_path}\")\n",
    "\n",
    "class CSVProcessorApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"CSV Column Standardizer (Enhanced Encoding Support)\")\n",
    "        self.root.geometry(\"700x500\")\n",
    "        \n",
    "        # Configure grid weight\n",
    "        self.root.grid_columnconfigure(0, weight=1)\n",
    "        self.root.grid_rowconfigure(3, weight=1)\n",
    "        \n",
    "        # Create file selection frame\n",
    "        file_frame = ttk.LabelFrame(root, text=\"File Selection\")\n",
    "        file_frame.grid(row=0, column=0, padx=10, pady=10, sticky=\"ew\")\n",
    "        file_frame.grid_columnconfigure(1, weight=1)\n",
    "        \n",
    "        # File list\n",
    "        self.file_listbox = tk.Listbox(file_frame, height=5, width=70, selectmode=tk.MULTIPLE)\n",
    "        self.file_listbox.grid(row=0, column=0, columnspan=2, padx=10, pady=5, sticky=\"ew\")\n",
    "        \n",
    "        # Add scrollbar to listbox\n",
    "        scrollbar = ttk.Scrollbar(file_frame, orient=\"vertical\", command=self.file_listbox.yview)\n",
    "        scrollbar.grid(row=0, column=2, pady=5, sticky=\"ns\")\n",
    "        self.file_listbox.configure(yscrollcommand=scrollbar.set)\n",
    "        \n",
    "        # File selection buttons\n",
    "        add_button = ttk.Button(file_frame, text=\"Add CSV Files\", command=self.add_files)\n",
    "        add_button.grid(row=1, column=0, padx=5, pady=5, sticky=\"w\")\n",
    "        \n",
    "        clear_button = ttk.Button(file_frame, text=\"Clear Selection\", command=self.clear_files)\n",
    "        clear_button.grid(row=1, column=1, padx=5, pady=5, sticky=\"e\")\n",
    "        \n",
    "        # Output directory frame\n",
    "        output_frame = ttk.LabelFrame(root, text=\"Output Directory\")\n",
    "        output_frame.grid(row=1, column=0, padx=10, pady=10, sticky=\"ew\")\n",
    "        output_frame.grid_columnconfigure(0, weight=1)\n",
    "        \n",
    "        self.output_dir_var = tk.StringVar(value=os.getcwd())\n",
    "        output_entry = ttk.Entry(output_frame, textvariable=self.output_dir_var, width=70)\n",
    "        output_entry.grid(row=0, column=0, padx=5, pady=5, sticky=\"ew\")\n",
    "        \n",
    "        browse_button = ttk.Button(output_frame, text=\"Browse...\", command=self.browse_output_dir)\n",
    "        browse_button.grid(row=0, column=1, padx=5, pady=5)\n",
    "        \n",
    "        # Process button\n",
    "        process_button = ttk.Button(root, text=\"Process Files\", command=self.process_files)\n",
    "        process_button.grid(row=2, column=0, padx=10, pady=5)\n",
    "        \n",
    "        # Log output area\n",
    "        log_frame = ttk.LabelFrame(root, text=\"Processing Log\")\n",
    "        log_frame.grid(row=3, column=0, padx=10, pady=10, sticky=\"nsew\")\n",
    "        log_frame.grid_columnconfigure(0, weight=1)\n",
    "        log_frame.grid_rowconfigure(0, weight=1)\n",
    "        \n",
    "        self.log_text = scrolledtext.ScrolledText(log_frame, width=80, height=15, wrap=tk.WORD)\n",
    "        self.log_text.grid(row=0, column=0, padx=5, pady=5, sticky=\"nsew\")\n",
    "        \n",
    "        # Status bar\n",
    "        self.status_var = tk.StringVar(value=\"Ready\")\n",
    "        status_bar = ttk.Label(root, textvariable=self.status_var, relief=tk.SUNKEN, anchor=tk.W)\n",
    "        status_bar.grid(row=4, column=0, sticky=\"ew\")\n",
    "        \n",
    "        # Store file paths\n",
    "        self.file_paths = []\n",
    "    \n",
    "    def add_files(self):\n",
    "        files = filedialog.askopenfilenames(\n",
    "            title=\"Select CSV files to process\",\n",
    "            filetypes=[(\"CSV files\", \"*.csv\"), (\"All files\", \"*.*\")]\n",
    "        )\n",
    "        \n",
    "        if files:\n",
    "            for file in files:\n",
    "                if file not in self.file_paths:\n",
    "                    self.file_paths.append(file)\n",
    "                    self.file_listbox.insert(tk.END, os.path.basename(file))\n",
    "            self.status_var.set(f\"{len(self.file_paths)} files selected\")\n",
    "    \n",
    "    def clear_files(self):\n",
    "        self.file_listbox.delete(0, tk.END)\n",
    "        self.file_paths = []\n",
    "        self.status_var.set(\"Ready\")\n",
    "    \n",
    "    def browse_output_dir(self):\n",
    "        directory = filedialog.askdirectory(title=\"Select output directory\")\n",
    "        if directory:\n",
    "            self.output_dir_var.set(directory)\n",
    "    \n",
    "    def log(self, message):\n",
    "        self.log_text.insert(tk.END, message + \"\\n\")\n",
    "        self.log_text.see(tk.END)\n",
    "        self.log_text.update_idletasks()\n",
    "    \n",
    "    def process_files(self):\n",
    "        if not self.file_paths:\n",
    "            messagebox.showwarning(\"Warning\", \"No CSV files selected!\")\n",
    "            return\n",
    "        \n",
    "        output_dir = self.output_dir_var.get()\n",
    "        if not output_dir:\n",
    "            messagebox.showwarning(\"Warning\", \"No output directory specified!\")\n",
    "            return\n",
    "        \n",
    "        # Clear log\n",
    "        self.log_text.delete(1.0, tk.END)\n",
    "        \n",
    "        self.log(f\"Processing {len(self.file_paths)} files...\")\n",
    "        self.log(f\"Output directory: {output_dir}\")\n",
    "        self.log(f\"Columns that will always be dropped: {', '.join(COLUMNS_TO_ALWAYS_DROP)}\")\n",
    "        self.log(\"Using enhanced encoding detection\")\n",
    "        \n",
    "        for i, file_path in enumerate(self.file_paths):\n",
    "            self.status_var.set(f\"Processing file {i+1} of {len(self.file_paths)}\")\n",
    "            try:\n",
    "                process_csv_file(file_path, output_dir, self.log)\n",
    "            except Exception as e:\n",
    "                self.log(f\"Error processing {os.path.basename(file_path)}: {str(e)}\")\n",
    "        \n",
    "        self.status_var.set(\"Processing complete\")\n",
    "        self.log(\"All files processed\")\n",
    "        messagebox.showinfo(\"Complete\", \"CSV processing completed successfully!\")\n",
    "\n",
    "def main():\n",
    "    # Check if chardet is installed\n",
    "    try:\n",
    "        import chardet\n",
    "    except ImportError:\n",
    "        print(\"The 'chardet' package is required for automatic encoding detection.\")\n",
    "        print(\"Please install it using: pip install chardet\")\n",
    "        print(\"Trying to continue without encoding detection...\")\n",
    "    \n",
    "    try:\n",
    "        root = tk.Tk()\n",
    "        app = CSVProcessorApp(root)\n",
    "        root.mainloop()\n",
    "    except Exception as e:\n",
    "        print(f\"Error starting GUI: {str(e)}\")\n",
    "        # Fall back to command line if GUI fails\n",
    "        fallback_command_line()\n",
    "\n",
    "def fallback_command_line():\n",
    "    \"\"\"Fallback to command line interface if GUI fails\"\"\"\n",
    "    print(\"GUI failed to start. Using command line interface instead.\")\n",
    "    print(\"\\nCSV Column Standardizer (Enhanced Encoding Support)\")\n",
    "    print(\"=================================================\")\n",
    "    print(f\"Columns that will always be dropped: {', '.join(COLUMNS_TO_ALWAYS_DROP)}\")\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\nEnter CSV file path (or multiple paths separated by commas), or 'q' to quit:\")\n",
    "        file_input = input(\"> \")\n",
    "        \n",
    "        if file_input.lower() == 'q':\n",
    "            break\n",
    "        \n",
    "        file_paths = [path.strip() for path in file_input.split(',')]\n",
    "        valid_paths = []\n",
    "        \n",
    "        for path in file_paths:\n",
    "            if os.path.isfile(path) and path.lower().endswith('.csv'):\n",
    "                valid_paths.append(path)\n",
    "            else:\n",
    "                print(f\"Invalid file path: {path}\")\n",
    "        \n",
    "        if not valid_paths:\n",
    "            print(\"No valid CSV files provided.\")\n",
    "            continue\n",
    "        \n",
    "        print(\"\\nEnter output directory (press Enter for current directory):\")\n",
    "        output_dir = input(\"> \").strip()\n",
    "        \n",
    "        if not output_dir:\n",
    "            output_dir = os.getcwd()\n",
    "        \n",
    "        print(f\"\\nProcessing {len(valid_paths)} file(s)...\")\n",
    "        \n",
    "        for file_path in valid_paths:\n",
    "            try:\n",
    "                process_csv_file(file_path, output_dir, print)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {os.path.basename(file_path)}: {str(e)}\")\n",
    "        \n",
    "        print(\"\\nProcessing complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcb361b0-65b5-42fd-b00a-63093b0bcef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEPS 7\n",
    "#Comment Merger\n",
    "\n",
    "import pandas as pd\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "import os\n",
    "import sys\n",
    "from tkinter import ttk\n",
    "from tkinter import scrolledtext\n",
    "import csv\n",
    "\n",
    "class CSVMergerApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"CSV Column Standardizer (Enhanced Emoji Support)\")\n",
    "        self.root.geometry(\"650x450\")\n",
    "        self.root.resizable(True, True)\n",
    "        \n",
    "        # Set window color to match screenshot (magenta)\n",
    "        self.root.configure(bg=\"#C71585\")\n",
    "        \n",
    "        # Create frames\n",
    "        self.create_widgets()\n",
    "        \n",
    "        # Initialize variables\n",
    "        self.file_paths = []\n",
    "        \n",
    "    def create_widgets(self):\n",
    "        # File Selection Frame\n",
    "        file_frame = tk.LabelFrame(self.root, text=\"File Selection\", bg=\"#C71585\", fg=\"white\")\n",
    "        file_frame.pack(fill=\"both\", expand=\"yes\", padx=10, pady=10)\n",
    "        \n",
    "        # File listbox\n",
    "        self.file_listbox = tk.Listbox(file_frame, width=70, height=5)\n",
    "        self.file_listbox.pack(padx=10, pady=10, fill=\"both\", expand=True)\n",
    "        \n",
    "        # Buttons frame\n",
    "        button_frame = tk.Frame(file_frame, bg=\"#C71585\")\n",
    "        button_frame.pack(fill=\"x\", padx=10, pady=5)\n",
    "        \n",
    "        # Add CSV Files button\n",
    "        self.add_button = tk.Button(button_frame, text=\"Add CSV Files\", command=self.add_files)\n",
    "        self.add_button.pack(side=\"left\", padx=5)\n",
    "        \n",
    "        # Clear Selection button\n",
    "        self.clear_button = tk.Button(button_frame, text=\"Clear Selection\", command=self.clear_selection)\n",
    "        self.clear_button.pack(side=\"right\", padx=5)\n",
    "        \n",
    "        # Output Directory Frame\n",
    "        output_frame = tk.LabelFrame(self.root, text=\"Output Directory\", bg=\"#C71585\", fg=\"white\")\n",
    "        output_frame.pack(fill=\"both\", padx=10, pady=10)\n",
    "        \n",
    "        # Output directory entry\n",
    "        self.output_path = tk.StringVar()\n",
    "        self.output_path.set(os.path.expanduser(\"~\"))\n",
    "        output_entry = tk.Entry(output_frame, textvariable=self.output_path, width=60)\n",
    "        output_entry.pack(side=\"left\", padx=(10, 5), pady=10, fill=\"x\", expand=True)\n",
    "        \n",
    "        # Browse button\n",
    "        browse_button = tk.Button(output_frame, text=\"Browse...\", command=self.browse_output)\n",
    "        browse_button.pack(side=\"right\", padx=(5, 10), pady=10)\n",
    "        \n",
    "        # Process Files button (centered)\n",
    "        process_frame = tk.Frame(self.root, bg=\"#C71585\")\n",
    "        process_frame.pack(fill=\"x\", padx=10, pady=5)\n",
    "        \n",
    "        self.process_button = tk.Button(process_frame, text=\"Process Files\", command=self.process_files)\n",
    "        self.process_button.pack(pady=5)\n",
    "        \n",
    "        # Processing Log Frame\n",
    "        log_frame = tk.LabelFrame(self.root, text=\"Processing Log\", bg=\"#C71585\", fg=\"white\")\n",
    "        log_frame.pack(fill=\"both\", expand=\"yes\", padx=10, pady=10)\n",
    "        \n",
    "        # Log text area\n",
    "        self.log_area = scrolledtext.ScrolledText(log_frame, width=70, height=8)\n",
    "        self.log_area.pack(padx=10, pady=10, fill=\"both\", expand=True)\n",
    "        \n",
    "        # Status bar\n",
    "        self.status_var = tk.StringVar()\n",
    "        self.status_var.set(\"Ready\")\n",
    "        status_bar = tk.Label(self.root, textvariable=self.status_var, bd=1, relief=tk.SUNKEN, anchor=tk.W)\n",
    "        status_bar.pack(side=tk.BOTTOM, fill=tk.X)\n",
    "    \n",
    "    def add_files(self):\n",
    "        \"\"\"Add CSV files to the list\"\"\"\n",
    "        files = filedialog.askopenfilenames(\n",
    "            title=\"Select CSV Files\",\n",
    "            filetypes=[(\"CSV Files\", \"*.csv\")]\n",
    "        )\n",
    "        \n",
    "        if files:\n",
    "            for file in files:\n",
    "                if file not in self.file_paths:\n",
    "                    self.file_paths.append(file)\n",
    "                    self.file_listbox.insert(tk.END, os.path.basename(file))\n",
    "            \n",
    "            self.status_var.set(f\"{len(self.file_paths)} files selected\")\n",
    "            self.log_message(f\"Added {len(files)} file(s) to the queue.\")\n",
    "    \n",
    "    def clear_selection(self):\n",
    "        \"\"\"Clear the file selection\"\"\"\n",
    "        self.file_paths = []\n",
    "        self.file_listbox.delete(0, tk.END)\n",
    "        self.status_var.set(\"Ready\")\n",
    "        self.log_message(\"File selection cleared.\")\n",
    "    \n",
    "    def browse_output(self):\n",
    "        \"\"\"Browse for output directory\"\"\"\n",
    "        output_dir = filedialog.askdirectory(\n",
    "            title=\"Select Output Directory\"\n",
    "        )\n",
    "        \n",
    "        if output_dir:\n",
    "            self.output_path.set(output_dir)\n",
    "            self.log_message(f\"Output directory set to: {output_dir}\")\n",
    "    \n",
    "    def log_message(self, message):\n",
    "        \"\"\"Add a message to the log area\"\"\"\n",
    "        self.log_area.insert(tk.END, message + \"\\n\")\n",
    "        self.log_area.see(tk.END)  # Scroll to the end\n",
    "    \n",
    "    def read_csv_manually(self, file_path):\n",
    "        \"\"\"Read CSV file manually to preserve emojis and special characters\"\"\"\n",
    "        rows = []\n",
    "        try:\n",
    "            # First attempt - UTF-8\n",
    "            with open(file_path, 'r', encoding='utf-8', newline='') as f:\n",
    "                reader = csv.reader(f)\n",
    "                headers = next(reader)  # Get header row\n",
    "                for row in reader:\n",
    "                    if row:  # Skip empty rows\n",
    "                        rows.append(row)\n",
    "        except UnicodeDecodeError:\n",
    "            # Second attempt - UTF-8-SIG (with BOM)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8-sig', newline='') as f:\n",
    "                    reader = csv.reader(f)\n",
    "                    headers = next(reader)  # Get header row\n",
    "                    for row in reader:\n",
    "                        if row:  # Skip empty rows\n",
    "                            rows.append(row)\n",
    "            except UnicodeDecodeError:\n",
    "                # Last attempt - cp1252 (Windows encoding)\n",
    "                with open(file_path, 'r', encoding='cp1252', newline='') as f:\n",
    "                    reader = csv.reader(f)\n",
    "                    headers = next(reader)  # Get header row\n",
    "                    for row in reader:\n",
    "                        if row:  # Skip empty rows\n",
    "                            rows.append(row)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(rows, columns=headers)\n",
    "        return df\n",
    "    \n",
    "    def extract_vid_from_filename(self, filename):\n",
    "        \"\"\"Extract Video ID from filename if possible\"\"\"\n",
    "        try:\n",
    "            # Try to extract Vid from filenames like \"Comments For Vid 6.csv\"\n",
    "            import re\n",
    "            match = re.search(r'Vid\\s*(\\d+)', filename)\n",
    "            if match:\n",
    "                return int(match.group(1))\n",
    "            return 1  # Default value\n",
    "        except:\n",
    "            return 1  # Default value\n",
    "    \n",
    "    def process_files(self):\n",
    "        \"\"\"Process the selected CSV files\"\"\"\n",
    "        if not self.file_paths:\n",
    "            messagebox.showinfo(\"File Selection\", \"Please select the comment CSV files to merge.\")\n",
    "            return\n",
    "            \n",
    "        # Ask user for the output file name\n",
    "        output_file = filedialog.asksaveasfilename(\n",
    "            title=\"Save Merged CSV As\",\n",
    "            defaultextension=\".csv\",\n",
    "            initialdir=self.output_path.get(),\n",
    "            filetypes=[(\"CSV Files\", \"*.csv\")]\n",
    "        )\n",
    "        \n",
    "        if not output_file:\n",
    "            self.log_message(\"Operation cancelled: No output file specified.\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Initialize an empty DataFrame for the merged data\n",
    "            merged_data = pd.DataFrame()\n",
    "            \n",
    "            # Keep track of the maximum Cid seen so far\n",
    "            max_cid = 0\n",
    "            \n",
    "            # Process each file\n",
    "            for file_path in self.file_paths:\n",
    "                # Get the filename for logging\n",
    "                filename = os.path.basename(file_path)\n",
    "                \n",
    "                self.log_message(f\"Processing {filename}...\")\n",
    "                self.status_var.set(f\"Processing {filename}\")\n",
    "                self.root.update_idletasks()\n",
    "                \n",
    "                # Extract Vid from filename\n",
    "                vid_from_filename = self.extract_vid_from_filename(filename)\n",
    "                \n",
    "                # Read the CSV file using our custom reader\n",
    "                try:\n",
    "                    df = self.read_csv_manually(file_path)\n",
    "                    \n",
    "                    # Ensure required columns exist\n",
    "                    required_cols = ['Vid', 'Cid', 'RepliesToId', 'Comments']\n",
    "                    for col in required_cols:\n",
    "                        if col not in df.columns:\n",
    "                            df[col] = \"\"\n",
    "                    \n",
    "                    # Fix the Vid column - use value from filename\n",
    "                    df['Vid'] = vid_from_filename\n",
    "                    \n",
    "                    # Convert Cid to integer safely\n",
    "                    df['Cid'] = df['Cid'].apply(lambda x: \n",
    "                        int(float(x)) if str(x).strip() and str(x).lower() != 'nan' \n",
    "                        else 0\n",
    "                    )\n",
    "                    \n",
    "                    # Create a mapping of old Cid to new Cid\n",
    "                    cid_mapping = {}\n",
    "                    new_cids = []\n",
    "                    \n",
    "                    # First pass: create the mapping\n",
    "                    for old_cid in df['Cid']:\n",
    "                        new_cid = max_cid + old_cid\n",
    "                        cid_mapping[old_cid] = new_cid\n",
    "                        new_cids.append(new_cid)\n",
    "                    \n",
    "                    # Update max_cid for the next file\n",
    "                    if new_cids:\n",
    "                        max_cid = max(new_cids)\n",
    "                    \n",
    "                    # Create a copy of the DataFrame to modify\n",
    "                    df_copy = df.copy()\n",
    "                    \n",
    "                    # Update the Cid values with the new ones\n",
    "                    df_copy['Cid'] = new_cids\n",
    "                    \n",
    "                    # Update the RepliesToId values (if not NaN)\n",
    "                    def update_replies_id(old_id):\n",
    "                        if pd.isna(old_id) or str(old_id).strip() == \"\" or str(old_id).lower() == 'nan':\n",
    "                            return \"\"\n",
    "                        try:\n",
    "                            # Convert to int if it's a number\n",
    "                            old_id_int = int(float(old_id))\n",
    "                            return str(cid_mapping.get(old_id_int, old_id_int))\n",
    "                        except (ValueError, TypeError):\n",
    "                            # If can't convert to int, return as is\n",
    "                            return old_id\n",
    "                    \n",
    "                    df_copy['RepliesToId'] = df_copy['RepliesToId'].apply(update_replies_id)\n",
    "                    \n",
    "                    # Append to the merged DataFrame\n",
    "                    merged_data = pd.concat([merged_data, df_copy], ignore_index=True)\n",
    "                    \n",
    "                    self.log_message(f\"Completed processing {filename}\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    self.log_message(f\"Error processing {filename}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            # Write the merged data using CSV writer\n",
    "            with open(output_file, 'w', encoding='utf-8', newline='') as f:\n",
    "                writer = csv.writer(f, quoting=csv.QUOTE_MINIMAL)\n",
    "                writer.writerow(merged_data.columns)  # Write header\n",
    "                \n",
    "                # Write data rows\n",
    "                for _, row in merged_data.iterrows():\n",
    "                    writer.writerow(row.values)\n",
    "            \n",
    "            self.log_message(f\"Successfully merged {len(self.file_paths)} files into {os.path.basename(output_file)}\")\n",
    "            self.status_var.set(\"Merge completed successfully\")\n",
    "            \n",
    "            messagebox.showinfo(\"Success\", f\"Successfully merged {len(self.file_paths)} files into {os.path.basename(output_file)}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            error_message = f\"An error occurred: {str(e)}\"\n",
    "            self.log_message(error_message)\n",
    "            self.status_var.set(\"Error occurred\")\n",
    "            messagebox.showerror(\"Error\", error_message)\n",
    "            \n",
    "\n",
    "def main():\n",
    "    root = tk.Tk()\n",
    "    app = CSVMergerApp(root)\n",
    "    root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c0c07bc-3187-4001-866b-5cfba0956791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicate Detector\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, filedialog, messagebox, scrolledtext\n",
    "import pandas as pd\n",
    "import os\n",
    "import chardet\n",
    "\n",
    "class CSVDuplicateChecker:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"CSV Cid Duplicate Checker\")\n",
    "        self.root.geometry(\"700x500\")\n",
    "        \n",
    "        # Configure grid weight\n",
    "        self.root.grid_columnconfigure(0, weight=1)\n",
    "        self.root.grid_rowconfigure(3, weight=1)\n",
    "        \n",
    "        # Create file selection frame\n",
    "        file_frame = ttk.LabelFrame(root, text=\"File Selection\")\n",
    "        file_frame.grid(row=0, column=0, padx=10, pady=10, sticky=\"ew\")\n",
    "        file_frame.grid_columnconfigure(1, weight=1)\n",
    "        \n",
    "        # File list\n",
    "        self.file_listbox = tk.Listbox(file_frame, height=5, width=70, selectmode=tk.MULTIPLE)\n",
    "        self.file_listbox.grid(row=0, column=0, columnspan=2, padx=10, pady=5, sticky=\"ew\")\n",
    "        \n",
    "        # Add scrollbar to listbox\n",
    "        scrollbar = ttk.Scrollbar(file_frame, orient=\"vertical\", command=self.file_listbox.yview)\n",
    "        scrollbar.grid(row=0, column=2, pady=5, sticky=\"ns\")\n",
    "        self.file_listbox.configure(yscrollcommand=scrollbar.set)\n",
    "        \n",
    "        # File selection buttons\n",
    "        add_button = ttk.Button(file_frame, text=\"Add CSV Files\", command=self.add_files)\n",
    "        add_button.grid(row=1, column=0, padx=5, pady=5, sticky=\"w\")\n",
    "        \n",
    "        clear_button = ttk.Button(file_frame, text=\"Clear Selection\", command=self.clear_files)\n",
    "        clear_button.grid(row=1, column=1, padx=5, pady=5, sticky=\"e\")\n",
    "        \n",
    "        # Process button\n",
    "        check_button = ttk.Button(root, text=\"Check for Duplicates\", command=self.check_duplicates)\n",
    "        check_button.grid(row=2, column=0, padx=10, pady=5)\n",
    "        \n",
    "        # Log output area\n",
    "        log_frame = ttk.LabelFrame(root, text=\"Processing Log\")\n",
    "        log_frame.grid(row=3, column=0, padx=10, pady=10, sticky=\"nsew\")\n",
    "        log_frame.grid_columnconfigure(0, weight=1)\n",
    "        log_frame.grid_rowconfigure(0, weight=1)\n",
    "        \n",
    "        self.log_text = scrolledtext.ScrolledText(log_frame, width=80, height=15, wrap=tk.WORD)\n",
    "        self.log_text.grid(row=0, column=0, padx=5, pady=5, sticky=\"nsew\")\n",
    "        \n",
    "        # Status bar\n",
    "        self.status_var = tk.StringVar(value=\"Ready\")\n",
    "        status_bar = ttk.Label(root, textvariable=self.status_var, relief=tk.SUNKEN, anchor=tk.W)\n",
    "        status_bar.grid(row=4, column=0, sticky=\"ew\")\n",
    "        \n",
    "        # Store file paths\n",
    "        self.file_paths = []\n",
    "    \n",
    "    def add_files(self):\n",
    "        files = filedialog.askopenfilenames(\n",
    "            title=\"Select CSV files to check\",\n",
    "            filetypes=[(\"CSV files\", \"*.csv\"), (\"All files\", \"*.*\")]\n",
    "        )\n",
    "        \n",
    "        if files:\n",
    "            for file in files:\n",
    "                if file not in self.file_paths:\n",
    "                    self.file_paths.append(file)\n",
    "                    self.file_listbox.insert(tk.END, os.path.basename(file))\n",
    "            self.status_var.set(f\"{len(self.file_paths)} files selected\")\n",
    "    \n",
    "    def clear_files(self):\n",
    "        self.file_listbox.delete(0, tk.END)\n",
    "        self.file_paths = []\n",
    "        self.status_var.set(\"Ready\")\n",
    "    \n",
    "    def log(self, message):\n",
    "        self.log_text.insert(tk.END, message + \"\\n\")\n",
    "        self.log_text.see(tk.END)\n",
    "        self.log_text.update_idletasks()\n",
    "    \n",
    "    def detect_encoding(self, file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            result = chardet.detect(f.read())\n",
    "        return result['encoding']\n",
    "    \n",
    "    def check_duplicates(self):\n",
    "        if not self.file_paths:\n",
    "            messagebox.showwarning(\"Warning\", \"No CSV files selected!\")\n",
    "            return\n",
    "        \n",
    "        # Clear log\n",
    "        self.log_text.delete(1.0, tk.END)\n",
    "        \n",
    "        self.log(f\"Checking {len(self.file_paths)} files for duplicates in 'Cid' column...\")\n",
    "        \n",
    "        for i, file_path in enumerate(self.file_paths):\n",
    "            file_name = os.path.basename(file_path)\n",
    "            self.status_var.set(f\"Checking file {i+1} of {len(self.file_paths)}\")\n",
    "            \n",
    "            try:\n",
    "                # Detect encoding\n",
    "                encoding = self.detect_encoding(file_path)\n",
    "                self.log(f\"Detected encoding for {file_name}: {encoding}\")\n",
    "                \n",
    "                # Read CSV file\n",
    "                df = pd.read_csv(file_path, encoding=encoding)\n",
    "                \n",
    "                # Check if \"Cid\" column exists\n",
    "                if 'Cid' not in df.columns:\n",
    "                    self.log(f\"Error: Column 'Cid' not found in {file_name}\")\n",
    "                    messagebox.showerror(\"Error\", f\"Column 'Cid' not found in {file_name}\")\n",
    "                    continue\n",
    "                \n",
    "                # Check for duplicates\n",
    "                duplicates = df[df.duplicated('Cid', keep=False)]\n",
    "                duplicate_count = len(duplicates)\n",
    "                \n",
    "                if duplicate_count > 0:\n",
    "                    self.log(f\"Found {duplicate_count} duplicate entries in 'Cid' column of {file_name}\")\n",
    "                    duplicate_values = duplicates['Cid'].value_counts()\n",
    "                    self.log(\"Duplicate Cid values and their counts:\")\n",
    "                    for value, count in duplicate_values.items():\n",
    "                        self.log(f\"  Cid: {value} appears {count} times\")\n",
    "                    \n",
    "                    messagebox.showinfo(\"Duplicates Found\", \n",
    "                                       f\"Found {duplicate_count} duplicate entries in 'Cid' column of {file_name}\")\n",
    "                else:\n",
    "                    self.log(f\"No duplicates found in 'Cid' column of {file_name}\")\n",
    "                    messagebox.showinfo(\"No Duplicates\", \n",
    "                                       f\"No duplicates found in 'Cid' column of {file_name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.log(f\"Error checking {file_name}: {str(e)}\")\n",
    "                messagebox.showerror(\"Error\", f\"Error checking {file_name}: {str(e)}\")\n",
    "        \n",
    "        self.status_var.set(\"Checking complete\")\n",
    "        self.log(\"All files checked for duplicates\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = CSVDuplicateChecker(root)\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "29118200-a343-4b97-b910-503e87659bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\user\\anaconda3\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: keybert in c:\\users\\user\\anaconda3\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\user\\anaconda3\\lib\\site-packages (4.50.0)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\user\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: rich>=10.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from keybert) (13.7.1)\n",
      "Requirement already satisfied: sentence-transformers>=0.3.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from keybert) (3.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich>=10.4.0->keybert) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich>=10.4.0->keybert) (2.15.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (10.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   Vid  Cid  RepliesToId                                           Comments  \\\n",
       " 0  1.0    1          NaN  I get derealization all the time it’s truly a ...   \n",
       " 1  1.0    2          NaN  Embarrassingly, I went to the ER multiple time...   \n",
       " 2  1.0    3          NaN                    I feel it in my stomach. Nausea   \n",
       " 3  1.0    4          NaN  Sometimes its not just anxiety & there is actu...   \n",
       " 4  1.0    5          NaN                                  Dry mouth anyone?   \n",
       " \n",
       "           uniqueId                                        videoWebUrl  \n",
       " 0     corriningram  https://www.tiktok.com/@healinghumanity777/vid...  \n",
       " 1           ag1283  https://www.tiktok.com/@healinghumanity777/vid...  \n",
       " 2        karekita2  https://www.tiktok.com/@healinghumanity777/vid...  \n",
       " 3  lurleenthequeen  https://www.tiktok.com/@healinghumanity777/vid...  \n",
       " 4   jaydonpearson2  https://www.tiktok.com/@healinghumanity777/vid...  ,\n",
       "    Vid                Name                        NickName  \\\n",
       " 0    1  healinghumanity777                     Logan Cohen   \n",
       " 1    2   noori_ahlifarmasi            Noori | Ahli Farmasi   \n",
       " 2    3   alexhowardtherapy         Alex Howard - Therapist   \n",
       " 3    4         anxietyjosh                     anxietyjosh   \n",
       " 4    5              jpwhit  James Whittaker | Win the Day®   \n",
       " \n",
       "                                    ProfileUrl  \\\n",
       " 0  https://www.tiktok.com/@healinghumanity777   \n",
       " 1   https://www.tiktok.com/@noori_ahlifarmasi   \n",
       " 2   https://www.tiktok.com/@alexhowardtherapy   \n",
       " 3         https://www.tiktok.com/@anxietyjosh   \n",
       " 4              https://www.tiktok.com/@jpwhit   \n",
       " \n",
       "                                 TikTok_Transcription  \\\n",
       " 0  00:00:00.000 --> 00:00:03.753\\nAnxiety can mak...   \n",
       " 1  00:00:00.000 --> 00:00:04.135\\nThe first 9 sig...   \n",
       " 2  00:00:00.000 --> 00:00:02.510\\nHere are my 3 t...   \n",
       " 3  00:00:00.000 --> 00:00:00.860\\nHi, I'm Josh,\\n...   \n",
       " 4  00:00:00.000 --> 00:00:01.711\\nFor anxiety spe...   \n",
       " \n",
       "                                          TikTok_Link  \n",
       " 0  https://www.tiktok.com/@healinghumanity777/vid...  \n",
       " 1  https://www.tiktok.com/@noori_ahlifarmasi/vide...  \n",
       " 2  https://www.tiktok.com/@alexhowardtherapy/vide...  \n",
       " 3  https://www.tiktok.com/@anxietyjosh/video/7460...  \n",
       " 4  https://www.tiktok.com/@jpwhit/video/722717446...  )"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STEPS 8\n",
    "#Text Normalisation\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "#Step 1: Environment Setup\n",
    "!pip install pandas numpy openpyxl keybert transformers torch scikit-learn\n",
    "\n",
    "\n",
    "# Loading the Merged Comment data\n",
    "comments_df = pd.read_csv(\"G:/Other computers/My Laptop (1)/Pelajaran UiTM/Classes and Lectures (Semester 5)/FINAL YEAR PROJECT/Developments/Simplified Data/Merged Files/Merged Comments V3.csv\")\n",
    "\n",
    "# Loading the Video with Transcription data\n",
    "videos_df = pd.read_excel(\"G:/Other computers/My Laptop (1)/Pelajaran UiTM/Classes and Lectures (Semester 5)/FINAL YEAR PROJECT/Developments/Simplified Data/Transcriptions/Video with Transcription.xlsx\")\n",
    "\n",
    "#Inspecting Data\n",
    "comments_df.head()\n",
    "videos_df.head()\n",
    "\n",
    "# Handling Missing Values Section\n",
    "# Check missing values in comments\n",
    "print(comments_df.isnull().sum())\n",
    "\n",
    "# Remove missing values or fill with placeholder text\n",
    "comments_df.dropna(subset=['comments'], inplace=True)\n",
    "\n",
    "# Alternatively, to fill:\n",
    "# comments_df['comments'].fillna(\"No comment provided\", inplace=True)\n",
    "\n",
    "# Check missing values in videos\n",
    "print(videos_df.isnull().sum())\n",
    "\n",
    "videos_df.dropna(subset=['transcription'], inplace=True)\n",
    "\n",
    "# Removing duplicates Section\n",
    "# Remove duplicates from comments\n",
    "comments_df.drop_duplicates(subset=['comments'], inplace=True)\n",
    "\n",
    "# Remove duplicates from videos\n",
    "videos_df.drop_duplicates(subset=['transcription'], inplace=True)\n",
    "\n",
    "# Text Normalization & Cleaning\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()  # convert text to lowercase\n",
    "    text = re.sub(r'http\\S+', '', text)  # remove URLs\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # normalize whitespace\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to comments and transcriptions\n",
    "comments_df['clean_comments'] = comments_df['comments'].apply(clean_text)\n",
    "videos_df['clean_transcriptions'] = videos_df['transcription'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858c2677-5fe1-400e-804a-c886977ce626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
