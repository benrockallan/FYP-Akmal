{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e67182-0749-41ce-9592-620ee1f05261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\user\\anaconda3\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: keybert in c:\\users\\user\\anaconda3\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\user\\anaconda3\\lib\\site-packages (4.50.0)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\user\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: rich>=10.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from keybert) (13.7.1)\n",
      "Requirement already satisfied: sentence-transformers>=0.3.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from keybert) (3.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich>=10.4.0->keybert) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich>=10.4.0->keybert) (2.15.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (10.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.0)\n",
      "Vid             135\n",
      "Cid               0\n",
      "RepliesToId    5982\n",
      "Comments          0\n",
      "uniqueId          4\n",
      "videoWebUrl       0\n",
      "dtype: int64\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "['comments']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14364\\3859847329.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# Check missing values in comments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomments_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m# Remove missing values or fill with placeholder text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mcomments_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'comments'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# Alternatively, to fill:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# comments_df['comments'].fillna(\"No comment provided\", inplace=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001b[0m\n\u001b[0;32m   6666\u001b[0m             \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magg_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6667\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6668\u001b[0m             \u001b[0mcheck\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6669\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcheck\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6670\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6671\u001b[0m             \u001b[0magg_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0magg_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6672\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6673\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mthresh\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_default\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ['comments']"
     ]
    }
   ],
   "source": [
    "#Data Preprocessing And Cleaning\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#Step 1: Environment Setup\n",
    "!pip install pandas numpy openpyxl keybert transformers torch scikit-learn\n",
    "\n",
    "\n",
    "# Loading the Merged Comment data\n",
    "comments_df = pd.read_csv(\"G:/Other computers/My Laptop (1)/Pelajaran UiTM/Classes and Lectures (Semester 5)/FINAL YEAR PROJECT/Developments/Simplified Data/Merged Files/Merged Comments V3.csv\")\n",
    "\n",
    "# Loading the Video with Transcription data\n",
    "videos_df = pd.read_excel(\"G:/Other computers/My Laptop (1)/Pelajaran UiTM/Classes and Lectures (Semester 5)/FINAL YEAR PROJECT/Developments/Simplified Data/Transcriptions/Video with Transcription.xlsx\")\n",
    "\n",
    "#Inspecting Data\n",
    "comments_df.head()\n",
    "videos_df.head()\n",
    "\n",
    "# Step 2: Handling Missing Values Section\n",
    "# Check missing values in comments\n",
    "print(comments_df.isnull().sum())\n",
    "\n",
    "# Remove missing values or fill with placeholder text\n",
    "comments_df.dropna(subset=['comments'], inplace=True)\n",
    "\n",
    "# Alternatively, to fill:\n",
    "# comments_df['comments'].fillna(\"No comment provided\", inplace=True)\n",
    "\n",
    "# Check missing values in videos\n",
    "print(videos_df.isnull().sum())\n",
    "\n",
    "videos_df.dropna(subset=['transcription'], inplace=True)\n",
    "\n",
    "# Step 3: Removing duplicates Section\n",
    "# Remove duplicates from comments\n",
    "comments_df.drop_duplicates(subset=['comments'], inplace=True)\n",
    "\n",
    "# Remove duplicates from videos\n",
    "videos_df.drop_duplicates(subset=['transcription'], inplace=True)\n",
    "\n",
    "# Step4: Text Normalization & Cleaning\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()  # convert text to lowercase\n",
    "    text = re.sub(r'http\\S+', '', text)  # remove URLs\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # normalize whitespace\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to comments and transcriptions\n",
    "comments_df['clean_comments'] = comments_df['comments'].apply(clean_text)\n",
    "videos_df['clean_transcriptions'] = videos_df['transcription'].apply(clean_text)\n",
    "\n",
    "#Step 5: Tokenization\n",
    "# Tokenize the cleaned comments and transcriptions\n",
    "comments_df['tokens'] = comments_df['clean_comments'].apply(word_tokenize)\n",
    "videos_df['tokens'] = videos_df['clean_transcriptions'].apply(word_tokenize)\n",
    "\n",
    "# Step 6: Stopword Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define function for removing stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "comments_df['tokens_no_stopwords'] = comments_df['tokens'].apply(remove_stopwords)\n",
    "videos_df['tokens_no_stopwords'] = videos_df['tokens'].apply(remove_stopwords)\n",
    "\n",
    "#Step 7: Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "comments_df['lemmatized'] = comments_df['tokens_no_stopwords'].apply(lemmatize_tokens)\n",
    "videos_df['lemmatized'] = videos_df['tokens_no_stopwords'].apply(lemmatize_tokens)\n",
    "\n",
    "#Step 8: Preparing for Keyword Extraction (KeyBERT)\n",
    "comments_df['final_text'] = comments_df['lemmatized'].apply(lambda x: ' '.join(x))\n",
    "videos_df['final_text'] = videos_df['lemmatized'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "#Step 9: Save Preprocessed Data\n",
    "comments_df.to_csv('clean_comments.csv', index=False)\n",
    "videos_df.to_csv('clean_transcriptions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4753772a-ce8f-4e49-b156-eeb2eee0ff71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 'punkt' already exists.\n",
      "‚úÖ 'stopwords' already exists.\n",
      "‚è≥ Downloading 'wordnet'...\n",
      "‚è≥ Downloading 'omw-1.4'...\n",
      "\n",
      "üîç Verifying downloads...\n",
      "‚úÖ 'punkt' successfully downloaded and verified.\n",
      "‚úÖ 'stopwords' successfully downloaded and verified.\n",
      "‚ùå 'wordnet' download failed. Please try manually.\n",
      "‚ùå 'omw-1.4' download failed. Please try manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#STEP 9\n",
    "#Installing NLTK\n",
    "#Pass this point is successful one\n",
    "\n",
    "import nltk\n",
    "\n",
    "# List of essential NLTK resources\n",
    "resources = [\n",
    "    ('punkt', 'tokenizers/punkt'),\n",
    "    ('stopwords', 'corpora/stopwords'),\n",
    "    ('wordnet', 'corpora/wordnet'),\n",
    "    ('omw-1.4', 'corpora/omw-1.4')  # Optional, for extended lemmatization support\n",
    "]\n",
    "\n",
    "for resource_name, resource_path in resources:\n",
    "    try:\n",
    "        nltk.data.find(resource_path)\n",
    "        print(f\"‚úÖ '{resource_name}' already exists.\")\n",
    "    except LookupError:\n",
    "        print(f\"‚è≥ Downloading '{resource_name}'...\")\n",
    "        nltk.download(resource_name)\n",
    "\n",
    "# Verification\n",
    "print(\"\\nüîç Verifying downloads...\")\n",
    "for resource_name, resource_path in resources:\n",
    "    try:\n",
    "        nltk.data.find(resource_path)\n",
    "        print(f\"‚úÖ '{resource_name}' successfully downloaded and verified.\")\n",
    "    except LookupError:\n",
    "        print(f\"‚ùå '{resource_name}' download failed. Please try manually.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f414861-70e8-4e0b-894d-6b80b3671eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:230: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:230: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_4816\\2717259807.py:230: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  keep_pattern = f\"[^\\w\\s{re.escape(emoji_chars)}]\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\user\\anaconda3\\lib\\site-packages (2.14.1)\n",
      "Required packages found.\n"
     ]
    }
   ],
   "source": [
    "# STEPS 10\n",
    "# KeyBERT Implementation - Extract keywords\n",
    "\n",
    "!pip install emoji\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, ttk, messagebox, scrolledtext\n",
    "import threading\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keybert import KeyBERT\n",
    "import time\n",
    "import traceback\n",
    "import emoji # <-- Import the emoji library\n",
    "\n",
    "# Download NLTK resources (Consider doing this once outside the main script execution if possible)\n",
    "# Ensure NLTK data path is correctly set up if needed\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "try:\n",
    "    nltk.data.find('corpora/omw-1.4')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "\n",
    "class KeywordExtractionApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Keyword Extraction Tool\")\n",
    "        self.root.geometry(\"800x600\")\n",
    "        self.root.resizable(True, True)\n",
    "\n",
    "        # Initialize variables\n",
    "        self.comments_file_path = tk.StringVar()\n",
    "        self.videos_file_path = tk.StringVar()\n",
    "        self.output_dir = tk.StringVar()\n",
    "        self.output_dir.set(os.getcwd())\n",
    "        self.progress_var = tk.DoubleVar()\n",
    "        self.status_var = tk.StringVar()\n",
    "        self.status_var.set(\"Ready\")\n",
    "        self.processing_thread = None\n",
    "        self.stop_requested = False\n",
    "\n",
    "        # Configure the main frame\n",
    "        main_frame = ttk.Frame(root, padding=\"10\")\n",
    "        main_frame.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "        # Create the input file selection section\n",
    "        self.create_file_selection_section(main_frame)\n",
    "\n",
    "        # Create the settings section\n",
    "        self.create_settings_section(main_frame)\n",
    "\n",
    "        # Create the output section\n",
    "        self.create_output_section(main_frame)\n",
    "\n",
    "        # Create the log section\n",
    "        self.create_log_section(main_frame)\n",
    "\n",
    "        # Create the control buttons section\n",
    "        self.create_control_buttons(main_frame)\n",
    "\n",
    "        # Configure progress bar\n",
    "        self.progress_bar = ttk.Progressbar(main_frame, variable=self.progress_var, length=780)\n",
    "        self.progress_bar.pack(fill=tk.X, padx=10, pady=5)\n",
    "\n",
    "        # Configure status bar\n",
    "        status_bar = ttk.Label(main_frame, textvariable=self.status_var, relief=tk.SUNKEN, anchor=tk.W)\n",
    "        status_bar.pack(fill=tk.X, padx=10, pady=5)\n",
    "\n",
    "    def create_file_selection_section(self, parent):\n",
    "        file_frame = ttk.LabelFrame(parent, text=\"Input Files\", padding=\"10\")\n",
    "        file_frame.pack(fill=tk.X, padx=10, pady=5)\n",
    "\n",
    "        # Comments file selector\n",
    "        ttk.Label(file_frame, text=\"Comments File:\").grid(row=0, column=0, sticky=tk.W, padx=5, pady=5)\n",
    "        ttk.Entry(file_frame, textvariable=self.comments_file_path, width=60).grid(row=0, column=1, padx=5, pady=5)\n",
    "        ttk.Button(file_frame, text=\"Browse...\", command=self.browse_comments_file).grid(row=0, column=2, padx=5, pady=5)\n",
    "\n",
    "        # Videos file selector\n",
    "        ttk.Label(file_frame, text=\"Videos File:\").grid(row=1, column=0, sticky=tk.W, padx=5, pady=5)\n",
    "        ttk.Entry(file_frame, textvariable=self.videos_file_path, width=60).grid(row=1, column=1, padx=5, pady=5)\n",
    "        ttk.Button(file_frame, text=\"Browse...\", command=self.browse_videos_file).grid(row=1, column=2, padx=5, pady=5)\n",
    "\n",
    "    def create_settings_section(self, parent):\n",
    "        settings_frame = ttk.LabelFrame(parent, text=\"Processing Settings\", padding=\"10\")\n",
    "        settings_frame.pack(fill=tk.X, padx=10, pady=5)\n",
    "\n",
    "        # Comment column name\n",
    "        ttk.Label(settings_frame, text=\"Comment Column:\").grid(row=0, column=0, sticky=tk.W, padx=5, pady=5)\n",
    "        self.comment_column_var = tk.StringVar(value=\"Comments\") # Default value\n",
    "        ttk.Entry(settings_frame, textvariable=self.comment_column_var, width=20).grid(row=0, column=1, padx=5, pady=5)\n",
    "\n",
    "        # Transcription column name\n",
    "        ttk.Label(settings_frame, text=\"Transcription Column:\").grid(row=0, column=2, sticky=tk.W, padx=5, pady=5)\n",
    "        self.transcription_column_var = tk.StringVar(value=\"transcription\") # Default value\n",
    "        ttk.Entry(settings_frame, textvariable=self.transcription_column_var, width=20).grid(row=0, column=3, padx=5, pady=5)\n",
    "\n",
    "        # Top N keywords\n",
    "        ttk.Label(settings_frame, text=\"Number of Keywords:\").grid(row=1, column=0, sticky=tk.W, padx=5, pady=5)\n",
    "        self.top_n_var = tk.IntVar(value=5)\n",
    "        ttk.Spinbox(settings_frame, from_=1, to=20, textvariable=self.top_n_var, width=5).grid(row=1, column=1, padx=5, pady=5)\n",
    "\n",
    "        # Batch size\n",
    "        ttk.Label(settings_frame, text=\"Batch Size:\").grid(row=1, column=2, sticky=tk.W, padx=5, pady=5)\n",
    "        self.batch_size_var = tk.IntVar(value=100)\n",
    "        ttk.Spinbox(settings_frame, from_=10, to=500, textvariable=self.batch_size_var, width=5).grid(row=1, column=3, padx=5, pady=5)\n",
    "\n",
    "    def create_output_section(self, parent):\n",
    "        output_frame = ttk.LabelFrame(parent, text=\"Output Settings\", padding=\"10\")\n",
    "        output_frame.pack(fill=tk.X, padx=10, pady=5)\n",
    "\n",
    "        # Output directory selector\n",
    "        ttk.Label(output_frame, text=\"Output Directory:\").grid(row=0, column=0, sticky=tk.W, padx=5, pady=5)\n",
    "        ttk.Entry(output_frame, textvariable=self.output_dir, width=60).grid(row=0, column=1, padx=5, pady=5)\n",
    "        ttk.Button(output_frame, text=\"Browse...\", command=self.browse_output_dir).grid(row=0, column=2, padx=5, pady=5)\n",
    "\n",
    "    def create_log_section(self, parent):\n",
    "        log_frame = ttk.LabelFrame(parent, text=\"Processing Log\", padding=\"10\")\n",
    "        log_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)\n",
    "\n",
    "        # Log text area\n",
    "        self.log_text = scrolledtext.ScrolledText(log_frame, wrap=tk.WORD, width=80, height=10)\n",
    "        self.log_text.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        self.log_text.config(state=tk.DISABLED)\n",
    "\n",
    "    def create_control_buttons(self, parent):\n",
    "        button_frame = ttk.Frame(parent)\n",
    "        button_frame.pack(fill=tk.X, padx=10, pady=5)\n",
    "\n",
    "        # Control buttons\n",
    "        self.start_button = ttk.Button(button_frame, text=\"Start Processing\", command=self.start_processing)\n",
    "        self.start_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.stop_button = ttk.Button(button_frame, text=\"Stop\", command=self.stop_processing, state=tk.DISABLED)\n",
    "        self.stop_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        ttk.Button(button_frame, text=\"Exit\", command=self.root.destroy).pack(side=tk.RIGHT, padx=5)\n",
    "\n",
    "    def browse_comments_file(self):\n",
    "        filepath = filedialog.askopenfilename(\n",
    "            filetypes=[(\"CSV files\", \"*.csv\"), (\"Excel files\", \"*.xlsx;*.xls\"), (\"All files\", \"*.*\")]\n",
    "        )\n",
    "        if filepath:\n",
    "            self.comments_file_path.set(filepath)\n",
    "\n",
    "    def browse_videos_file(self):\n",
    "        filepath = filedialog.askopenfilename(\n",
    "            filetypes=[(\"Excel files\", \"*.xlsx;*.xls\"), (\"CSV files\", \"*.csv\"), (\"All files\", \"*.*\")]\n",
    "        )\n",
    "        if filepath:\n",
    "            self.videos_file_path.set(filepath)\n",
    "\n",
    "    def browse_output_dir(self):\n",
    "        dirpath = filedialog.askdirectory()\n",
    "        if dirpath:\n",
    "            self.output_dir.set(dirpath)\n",
    "\n",
    "    def log(self, message):\n",
    "        self.root.after(0, self._log, message)\n",
    "\n",
    "    def _log(self, message):\n",
    "        self.log_text.config(state=tk.NORMAL)\n",
    "        self.log_text.insert(tk.END, f\"{message}\\n\")\n",
    "        self.log_text.see(tk.END)\n",
    "        self.log_text.config(state=tk.DISABLED)\n",
    "\n",
    "    def update_status(self, message):\n",
    "        self.status_var.set(message)\n",
    "\n",
    "    def update_progress(self, value):\n",
    "        self.progress_var.set(value)\n",
    "\n",
    "    def start_processing(self):\n",
    "        if not self.comments_file_path.get() and not self.videos_file_path.get():\n",
    "            messagebox.showerror(\"Error\", \"Please select at least one input file.\")\n",
    "            return\n",
    "\n",
    "        self.start_button.config(state=tk.DISABLED)\n",
    "        self.stop_button.config(state=tk.NORMAL)\n",
    "        self.stop_requested = False\n",
    "\n",
    "        self.log_text.config(state=tk.NORMAL)\n",
    "        self.log_text.delete(1.0, tk.END)\n",
    "        self.log_text.config(state=tk.DISABLED)\n",
    "\n",
    "        self.processing_thread = threading.Thread(target=self.process_files)\n",
    "        self.processing_thread.daemon = True\n",
    "        self.processing_thread.start()\n",
    "\n",
    "    def stop_processing(self):\n",
    "        self.stop_requested = True\n",
    "        self.log(\"Stop requested. Waiting for current batch to complete...\")\n",
    "        self.update_status(\"Stopping...\")\n",
    "\n",
    "    def process_files(self):\n",
    "        try:\n",
    "            self.log(\"Starting keyword extraction process...\")\n",
    "            self.update_status(\"Initializing...\")\n",
    "\n",
    "            comment_column = self.comment_column_var.get()\n",
    "            transcription_column = self.transcription_column_var.get()\n",
    "            top_n = self.top_n_var.get()\n",
    "            batch_size = self.batch_size_var.get()\n",
    "            output_dir = self.output_dir.get()\n",
    "\n",
    "            # --- Text cleaning and processing functions ---\n",
    "            self.log(\"Setting up text processing functions...\")\n",
    "\n",
    "            # Get all emoji characters known by the emoji library\n",
    "            emoji_chars = \"\".join(emoji.EMOJI_DATA.keys())\n",
    "            # Create a regex pattern to keep word characters, whitespace, AND emojis\n",
    "            # This pattern excludes characters that are NOT word chars, whitespace, or in the emoji set\n",
    "            keep_pattern = f\"[^\\w\\s{re.escape(emoji_chars)}]\"\n",
    "\n",
    "            def clean_text(text):\n",
    "                if not isinstance(text, str):\n",
    "                    return \"\"\n",
    "                text = text.lower()\n",
    "                # Remove URLs first\n",
    "                text = re.sub(r'http\\S+', '', text)\n",
    "                # Remove digits\n",
    "                text = re.sub(r'\\d+', '', text)\n",
    "                # Remove punctuation EXCEPT emojis\n",
    "                text = re.sub(keep_pattern, '', text, flags=re.UNICODE)\n",
    "                # Remove extra whitespace\n",
    "                text = re.sub(r'\\s+', ' ', text).strip()\n",
    "                return text\n",
    "\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "            def process_text(text):\n",
    "                # Tokenize (word_tokenize might need refinement for complex emoji handling if issues arise)\n",
    "                tokens = word_tokenize(text)\n",
    "                # Remove stopwords (emojis are unlikely to be stopwords)\n",
    "                tokens_no_stopwords = [word for word in tokens if word not in stop_words]\n",
    "                # Lemmatize (lemmatizer should ignore emojis)\n",
    "                lemmatized = [lemmatizer.lemmatize(token) for token in tokens_no_stopwords]\n",
    "                return ' '.join(lemmatized)\n",
    "\n",
    "            # --- Load data (Ensure error handling for file loading) ---\n",
    "            comments_df = None\n",
    "            videos_df = None\n",
    "\n",
    "            try:\n",
    "                if self.comments_file_path.get():\n",
    "                    self.log(f\"Loading comments data from {self.comments_file_path.get()}...\")\n",
    "                    self.update_status(\"Loading comments data...\")\n",
    "                    if self.comments_file_path.get().lower().endswith('.csv'):\n",
    "                        comments_df = pd.read_csv(self.comments_file_path.get())\n",
    "                    else:\n",
    "                        comments_df = pd.read_excel(self.comments_file_path.get())\n",
    "                    self.log(f\"Successfully loaded comments data with {len(comments_df)} rows\")\n",
    "                    self.log(f\"Columns: {comments_df.columns.tolist()}\")\n",
    "                    # Validate comment column\n",
    "                    if comment_column not in comments_df.columns:\n",
    "                         self.log(f\"Warning: '{comment_column}' not found. Available: {comments_df.columns.tolist()}. Attempting fallback...\")\n",
    "                         # Add fallback logic or error out\n",
    "                         # For now, let's assume it fails if not found after potential fallback\n",
    "                         potential_columns = [col for col in comments_df.columns if 'comment' in col.lower()]\n",
    "                         if potential_columns:\n",
    "                             comment_column = potential_columns[0]\n",
    "                             self.log(f\"Using '{comment_column}' instead.\")\n",
    "                         else:\n",
    "                             raise ValueError(f\"Specified comment column '{self.comment_column_var.get()}' not found.\")\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                self.log(f\"Error loading comments file: {e}\")\n",
    "                messagebox.showerror(\"File Load Error\", f\"Failed to load comments file: {e}\")\n",
    "                comments_df = None # Ensure it's None if loading fails\n",
    "\n",
    "            try:\n",
    "                if self.videos_file_path.get():\n",
    "                    self.log(f\"Loading videos data from {self.videos_file_path.get()}...\")\n",
    "                    self.update_status(\"Loading videos data...\")\n",
    "                    if self.videos_file_path.get().lower().endswith('.csv'):\n",
    "                        videos_df = pd.read_csv(self.videos_file_path.get())\n",
    "                    else:\n",
    "                        videos_df = pd.read_excel(self.videos_file_path.get())\n",
    "                    self.log(f\"Successfully loaded videos data with {len(videos_df)} rows\")\n",
    "                    self.log(f\"Columns: {videos_df.columns.tolist()}\")\n",
    "                     # Validate transcription column\n",
    "                    if transcription_column not in videos_df.columns:\n",
    "                        self.log(f\"Warning: '{transcription_column}' not found. Available: {videos_df.columns.tolist()}. Attempting fallback...\")\n",
    "                        # Add fallback logic or error out\n",
    "                        potential_columns = [col for col in videos_df.columns if 'transcript' in col.lower() or 'text' in col.lower()]\n",
    "                        if potential_columns:\n",
    "                            transcription_column = potential_columns[0]\n",
    "                            self.log(f\"Using '{transcription_column}' instead.\")\n",
    "                        else:\n",
    "                            raise ValueError(f\"Specified transcription column '{self.transcription_column_var.get()}' not found.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                self.log(f\"Error loading videos file: {e}\")\n",
    "                messagebox.showerror(\"File Load Error\", f\"Failed to load videos file: {e}\")\n",
    "                videos_df = None # Ensure it's None if loading fails\n",
    "\n",
    "\n",
    "            # --- Initialize KeyBERT ---\n",
    "            self.log(\"Initializing KeyBERT model...\")\n",
    "            self.update_status(\"Initializing KeyBERT model...\")\n",
    "            kw_model = KeyBERT()\n",
    "\n",
    "            # --- Function to extract keywords ---\n",
    "            def extract_keywords(text, top_n_kw=5):\n",
    "                if not text or len(text.split()) < 3: # Check if text is too short\n",
    "                    return []\n",
    "                try:\n",
    "                    # Pass the *cleaned but not processed* text for better context preservation if needed\n",
    "                    # Or pass processed_text if lemmatization/stopword removal is desired *before* keyword extraction\n",
    "                    keywords = kw_model.extract_keywords(\n",
    "                        text, # Using the processed_text here\n",
    "                        keyphrase_ngram_range=(1, 2),\n",
    "                        stop_words='english', # KeyBERT can handle its own stopwords\n",
    "                        use_mmr=True,\n",
    "                        diversity=0.7,\n",
    "                        top_n=top_n_kw\n",
    "                    )\n",
    "                    # Return list of (keyword, score) tuples\n",
    "                    return keywords if keywords else []\n",
    "                except Exception as e:\n",
    "                    self.log(f\"Error extracting keywords for text chunk: {e}\")\n",
    "                    # self.log(f\"Problematic text (first 100 chars): {text[:100]}\") # Optional: log problematic text\n",
    "                    return [] # Return empty list on error for this chunk\n",
    "\n",
    "            # --- Process comments if available ---\n",
    "            if comments_df is not None:\n",
    "                self.log(\"\\nProcessing comments data...\")\n",
    "                self.update_status(\"Processing comments data...\")\n",
    "\n",
    "                # Clean and preprocess text\n",
    "                self.log(\"Cleaning comments...\")\n",
    "                comments_df['clean_text'] = comments_df[comment_column].apply(clean_text)\n",
    "                self.log(\"Preprocessing comments (tokenize, stopwords, lemmatize)...\")\n",
    "                comments_df['processed_text'] = comments_df['clean_text'].apply(process_text)\n",
    "\n",
    "\n",
    "                # Extract keywords in batches\n",
    "                self.log(\"Extracting keywords from comments...\")\n",
    "                keywords_list_comments = []\n",
    "                total_comments = len(comments_df)\n",
    "\n",
    "                for i in range(0, total_comments, batch_size):\n",
    "                    if self.stop_requested:\n",
    "                        self.log(\"Comment processing stopped by user.\")\n",
    "                        break\n",
    "\n",
    "                    batch_end = min(i + batch_size, total_comments)\n",
    "                    current_batch_num = (i // batch_size) + 1\n",
    "                    total_batches = (total_comments + batch_size - 1) // batch_size\n",
    "\n",
    "                    self.log(f\"Processing comment batch {current_batch_num}/{total_batches} (rows {i+1} to {batch_end})\")\n",
    "                    self.update_status(f\"Processing comments: {batch_end}/{total_comments}\")\n",
    "\n",
    "                    # Use 'processed_text' for keyword extraction\n",
    "                    batch_texts = comments_df['processed_text'].iloc[i:batch_end]\n",
    "                    batch_keywords = [extract_keywords(text, top_n) for text in batch_texts]\n",
    "                    keywords_list_comments.extend(batch_keywords)\n",
    "\n",
    "                    progress = (batch_end / total_comments) * 50 # Comments take first 50%\n",
    "                    self.update_progress(progress)\n",
    "\n",
    "                if not self.stop_requested:\n",
    "                    comments_df['keywords'] = keywords_list_comments\n",
    "                    comments_df['keywords_only'] = comments_df['keywords'].apply(\n",
    "                        lambda kw_list: [k for k, _ in kw_list] if isinstance(kw_list, list) else []\n",
    "                    )\n",
    "\n",
    "                    output_path = os.path.join(output_dir, 'comments_with_keywords.csv')\n",
    "                    try:\n",
    "                        self.log(f\"Saving processed comments to {output_path}\")\n",
    "                        comments_df.to_csv(output_path, index=False, encoding='utf-8-sig') # Ensure UTF-8 for emojis\n",
    "                        self.log(\"Saved comments successfully.\")\n",
    "                        self.log(\"\\nSample keywords from comments:\")\n",
    "                        for idx in range(min(3, len(comments_df))):\n",
    "                             original_comment = comments_df[comment_column].iloc[idx]\n",
    "                             extracted_kws = comments_df['keywords'].iloc[idx]\n",
    "                             self.log(f\"Comment (Original): {str(original_comment)[:100]}...\")\n",
    "                             self.log(f\"Keywords: {extracted_kws}\")\n",
    "                             self.log(\"-\" * 20)\n",
    "                    except Exception as e:\n",
    "                         self.log(f\"Error saving comments file: {e}\")\n",
    "                         messagebox.showerror(\"File Save Error\", f\"Failed to save comments file: {e}\")\n",
    "\n",
    "\n",
    "            # --- Process videos if available ---\n",
    "            if videos_df is not None and not self.stop_requested:\n",
    "                self.log(\"\\nProcessing video transcription data...\")\n",
    "                self.update_status(\"Processing video transcriptions...\")\n",
    "\n",
    "                self.log(\"Cleaning transcriptions...\")\n",
    "                videos_df['clean_text'] = videos_df[transcription_column].apply(clean_text)\n",
    "                self.log(\"Preprocessing transcriptions (tokenize, stopwords, lemmatize)...\")\n",
    "                videos_df['processed_text'] = videos_df['clean_text'].apply(process_text)\n",
    "\n",
    "                self.log(\"Extracting keywords from transcriptions...\")\n",
    "                keywords_list_videos = []\n",
    "                total_videos = len(videos_df)\n",
    "\n",
    "                for i in range(0, total_videos, batch_size):\n",
    "                     if self.stop_requested:\n",
    "                         self.log(\"Video processing stopped by user.\")\n",
    "                         break\n",
    "\n",
    "                     batch_end = min(i + batch_size, total_videos)\n",
    "                     current_batch_num = (i // batch_size) + 1\n",
    "                     total_batches = (total_videos + batch_size - 1) // batch_size\n",
    "\n",
    "                     self.log(f\"Processing transcription batch {current_batch_num}/{total_batches} (rows {i+1} to {batch_end})\")\n",
    "                     self.update_status(f\"Processing transcriptions: {batch_end}/{total_videos}\")\n",
    "\n",
    "                     batch_texts = videos_df['processed_text'].iloc[i:batch_end]\n",
    "                     batch_keywords = [extract_keywords(text, top_n) for text in batch_texts]\n",
    "                     keywords_list_videos.extend(batch_keywords)\n",
    "\n",
    "                     # Update progress (videos take second 50% or full 100% if no comments)\n",
    "                     base_progress = 50 if comments_df is not None else 0\n",
    "                     progress = base_progress + (batch_end / total_videos) * (100 - base_progress)\n",
    "                     self.update_progress(progress)\n",
    "\n",
    "\n",
    "                if not self.stop_requested:\n",
    "                    videos_df['keywords'] = keywords_list_videos\n",
    "                    videos_df['keywords_only'] = videos_df['keywords'].apply(\n",
    "                        lambda kw_list: [k for k, _ in kw_list] if isinstance(kw_list, list) else []\n",
    "                    )\n",
    "\n",
    "                    output_path = os.path.join(output_dir, 'videos_with_keywords.csv')\n",
    "                    try:\n",
    "                        self.log(f\"Saving processed videos to {output_path}\")\n",
    "                        videos_df.to_csv(output_path, index=False, encoding='utf-8-sig') # Ensure UTF-8 for emojis\n",
    "                        self.log(\"Saved videos successfully.\")\n",
    "                        self.log(\"\\nSample keywords from videos:\")\n",
    "                        for idx in range(min(3, len(videos_df))):\n",
    "                             original_transcript = videos_df[transcription_column].iloc[idx]\n",
    "                             extracted_kws = videos_df['keywords'].iloc[idx]\n",
    "                             self.log(f\"Transcription (Original): {str(original_transcript)[:100]}...\")\n",
    "                             self.log(f\"Keywords: {extracted_kws}\")\n",
    "                             self.log(\"-\" * 20)\n",
    "                    except Exception as e:\n",
    "                         self.log(f\"Error saving videos file: {e}\")\n",
    "                         messagebox.showerror(\"File Save Error\", f\"Failed to save videos file: {e}\")\n",
    "\n",
    "\n",
    "            # --- Completion message ---\n",
    "            if self.stop_requested:\n",
    "                self.update_status(\"Processing stopped by user\")\n",
    "                self.log(\"Processing stopped before completion.\")\n",
    "            else:\n",
    "                self.update_status(\"Processing completed\")\n",
    "                self.log(\"Keyword extraction completed successfully!\")\n",
    "                self.update_progress(100)\n",
    "                messagebox.showinfo(\"Success\", \"Keyword extraction completed successfully!\")\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "             self.log(f\"Error: Input file not found - {e}\")\n",
    "             self.update_status(\"Error: File not found\")\n",
    "             messagebox.showerror(\"File Not Found\", f\"Could not find the specified file: {e}\")\n",
    "        except ValueError as e: # Catch column errors\n",
    "             self.log(f\"Configuration Error: {e}\")\n",
    "             self.update_status(\"Error: Column not found\")\n",
    "             messagebox.showerror(\"Configuration Error\", str(e))\n",
    "        except Exception as e:\n",
    "            self.log(f\"An unexpected error occurred during processing: {e}\")\n",
    "            self.log(traceback.format_exc()) # Log the full traceback for debugging\n",
    "            self.update_status(\"Error occurred\")\n",
    "            messagebox.showerror(\"Error\", f\"An unexpected error occurred: {str(e)}\")\n",
    "\n",
    "        finally:\n",
    "            # Ensure buttons are reset in the main thread\n",
    "            self.root.after(0, lambda: self.start_button.config(state=tk.NORMAL))\n",
    "            self.root.after(0, lambda: self.stop_button.config(state=tk.DISABLED))\n",
    "\n",
    "\n",
    "# Main application launcher\n",
    "if __name__ == \"__main__\":\n",
    "    # Check for required packages before starting GUI\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import nltk\n",
    "        from keybert import KeyBERT\n",
    "        import emoji # Check for emoji library too\n",
    "        print(\"Required packages found.\")\n",
    "    except ImportError as e:\n",
    "        print(f\"Error: Missing required package: {e}\")\n",
    "        print(\"Please install the required packages. You might need:\")\n",
    "        print(\"pip install pandas numpy nltk keybert transformers torch scikit-learn openpyxl emoji\")\n",
    "        # Show error in a simple Tkinter window if possible, as main GUI might not start\n",
    "        try:\n",
    "            root_check = tk.Tk()\n",
    "            root_check.withdraw() # Hide the main window\n",
    "            messagebox.showerror(\"Missing Packages\",\n",
    "                                 f\"Missing required package: {e.name}.\\n\\nPlease install packages, e.g.,:\\n\"\n",
    "                                 \"pip install pandas numpy nltk keybert transformers torch scikit-learn openpyxl emoji\")\n",
    "            root_check.destroy()\n",
    "        except tk.TclError:\n",
    "             # Handle cases where Tkinter itself might be missing or has issues\n",
    "            print(\"Tkinter error, cannot display GUI message box.\")\n",
    "        sys.exit(1) # Exit if packages are missing\n",
    "\n",
    "\n",
    "    # Pre-download NLTK (optional but good practice) - simplified check\n",
    "    # print(\"Checking NLTK resources...\")\n",
    "    # resources = ['punkt', 'stopwords', 'wordnet', 'omw-1.4']\n",
    "    # for resource in resources:\n",
    "    #     try:\n",
    "    #         nltk.data.find(f'corpora/{resource}' if resource != 'punkt' else f'tokenizers/{resource}')\n",
    "    #     except LookupError:\n",
    "    #         print(f\"Downloading NLTK resource: {resource}...\")\n",
    "    #         nltk.download(resource, quiet=True) # Download quietly\n",
    "\n",
    "    # Create and run the application\n",
    "    root = tk.Tk()\n",
    "    app = KeywordExtractionApp(root)\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "916e6b84-919e-44c2-806c-a0841cd22ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\user\\anaconda3\\lib\\site-packages (4.50.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Processing keywords: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 786/786 [02:48<00:00,  4.66it/s]\n",
      "Processing keywords: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Video Keywords Relevant to Input:\n",
      "                                             Keyword  Similarity Score\n",
      "0  [('want antidepressant', 0.6271), ('feeling aw...          0.667951\n",
      "1  [('drs scared', 0.7271), ('im scared', 0.5479)...          0.653591\n",
      "2  [('diagnosed anxiety', 0.6303), ('im heartbrok...          0.651593\n",
      "3  [('scared feel', 0.5524), ('im starting', 0.43...          0.651471\n",
      "4  [('im lonely', 0.6674), ('feeling everyday', 0...          0.649974\n",
      "5  [('peace depression', 0.7632), ('help peace', ...          0.649039\n",
      "6  [('depression anxiety', 0.5873), ('dad like', ...          0.648247\n",
      "7  [('zoloft anxiety', 0.6363), ('diagnosed adhd'...          0.648160\n",
      "8  [('depression disability', 0.6849), ('tired li...          0.647352\n",
      "9  [('sayimgi understand', 0.6682), ('depression ...          0.647301\n",
      "\n",
      "Top Comment Keywords Relevant to Input:\n",
      "                                             Keyword  Similarity Score\n",
      "0  [('anxiety feel', 0.6967), ('lightheadedness d...          0.637515\n",
      "1  [('depressed depression', 0.5894), ('hurt know...          0.633980\n",
      "2  [('dealt depression', 0.6746), ('fifth feeling...          0.630641\n",
      "3  [('depression depression', 0.659), ('understan...          0.628710\n",
      "4  [('dealing depression', 0.6028), ('need people...          0.623972\n",
      "5  [('possible depression', 0.7114), ('hi dr', 0....          0.623553\n",
      "6  [('feeling exhausted', 0.4799), ('coping mecha...          0.622397\n",
      "7  [('depression', 0.5353), ('change sleep', 0.34...          0.622216\n",
      "8  [('tip therapist', 0.4504), ('health anxiety',...          0.621781\n",
      "9  [('health anxiety', 0.5981), ('sensation numbe...          0.618901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#STEPS 11 - Implementing BERT model\n",
    "#Try Accuracy Testing MentalBERT\n",
    "\n",
    "# Install dependencies (uncomment in Colab or terminal)\n",
    "# !pip install transformers torch pandas scikit-learn\n",
    "\n",
    "!pip install transformers\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Your Hugging Face token\n",
    "hf_token = \"hf_...\"  # Replace with your actual token\n",
    "\n",
    "# Load MentalBERT model and tokenizer using the token\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"mental/mental-bert-base-uncased\", token=hf_token)\n",
    "    model = AutoModel.from_pretrained(\"mental/mental-bert-base-uncased\", token=hf_token)\n",
    "except TypeError:\n",
    "    print(\"Token argument not accepted or login required. Ensure you are logged in via CLI or notebook_login().\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"mental/mental-bert-base-uncased\")\n",
    "    model = AutoModel.from_pretrained(\"mental/mental-bert-base-uncased\")\n",
    "\n",
    "# Function to generate embedding using MentalBERT\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "# Function to generate embeddings for a batch of texts\n",
    "def get_batch_embeddings(texts):\n",
    "    # Ensure all texts are strings before tokenizing\n",
    "    texts = [str(text) for text in texts]  # Convert all to strings\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "# Function to rank keywords against a user input with batch processing\n",
    "def rank_keywords_batch(df, keyword_column, user_input, batch_size=10):\n",
    "    user_embedding = get_embedding(user_input)\n",
    "    rankings = []\n",
    "\n",
    "    # Process keywords in batches with tqdm progress bar\n",
    "    for i in tqdm(range(0, len(df[keyword_column]), batch_size), desc=\"Processing keywords\"):\n",
    "        # Get the current batch of keywords\n",
    "        keywords_batch = df[keyword_column].dropna().unique()[i:i+batch_size]\n",
    "\n",
    "        # Ensure the batch is not empty\n",
    "        if not keywords_batch.size: # Changed this line\n",
    "            continue\n",
    "\n",
    "        # Convert all keywords to string before embedding\n",
    "        keywords_batch = [str(keyword) for keyword in keywords_batch]\n",
    "\n",
    "        try:\n",
    "            keyword_embeddings = get_batch_embeddings(keywords_batch)\n",
    "\n",
    "            # Move embeddings to CPU if necessary before calculating similarity\n",
    "            keyword_embeddings = keyword_embeddings.cpu().numpy()\n",
    "            normalized_user_embedding = normalize(user_embedding.cpu().numpy().reshape(1, -1)) #Added normalization and reshape\n",
    "            normalized_keyword_embeddings = normalize(keyword_embeddings)\n",
    "            similarities = cosine_similarity(normalized_user_embedding, normalized_keyword_embeddings)\n",
    "\n",
    "            # Rank keywords based on similarity\n",
    "            for keyword, score in zip(keywords_batch, similarities[0]): # changed similarities to similarities[0]\n",
    "                rankings.append((keyword, score))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch of keywords: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return sorted(rankings, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Load your CSV files (make sure paths are correct)\n",
    "videos_df = pd.read_csv(\"G:/Other computers/My Laptop (1)/Pelajaran UiTM/Classes and Lectures (Semester 6)/CSP650/Developments/Simplified Data/Testings/comments_with_keywords.csv\")\n",
    "comments_df = pd.read_csv(\"G:/Other computers/My Laptop (1)/Pelajaran UiTM/Classes and Lectures (Semester 6)/CSP650/Developments/Simplified Data/Testings/videos_with_keywords.csv\")\n",
    "\n",
    "# Clean up the data (remove any rows with missing 'keywords')\n",
    "videos_df = videos_df.dropna(subset=[\"keywords\"])\n",
    "comments_df = comments_df.dropna(subset=[\"keywords\"])\n",
    "\n",
    "# Input from user\n",
    "user_input = \"I'm feeling down\"\n",
    "\n",
    "# Rank keywords separately for videos and comments using batch processing\n",
    "video_ranks = rank_keywords_batch(videos_df, \"keywords\", user_input)\n",
    "comment_ranks = rank_keywords_batch(comments_df, \"keywords\", user_input)\n",
    "\n",
    "# Convert results to DataFrame and display\n",
    "video_results = pd.DataFrame(video_ranks, columns=[\"Keyword\", \"Similarity Score\"])\n",
    "comment_results = pd.DataFrame(comment_ranks, columns=[\"Keyword\", \"Similarity Score\"])\n",
    "\n",
    "print(\"\\nTop Video Keywords Relevant to Input:\")\n",
    "print(video_results.head(10))\n",
    "\n",
    "print(\"\\nTop Comment Keywords Relevant to Input:\")\n",
    "print(comment_results.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fbd844-cf92-472e-b30a-32a0e941a758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
