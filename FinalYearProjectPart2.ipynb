{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e67182-0749-41ce-9592-620ee1f05261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\user\\anaconda3\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: keybert in c:\\users\\user\\anaconda3\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\user\\anaconda3\\lib\\site-packages (4.50.0)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\user\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: rich>=10.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from keybert) (13.7.1)\n",
      "Requirement already satisfied: sentence-transformers>=0.3.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from keybert) (3.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich>=10.4.0->keybert) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich>=10.4.0->keybert) (2.15.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (10.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.0)\n",
      "Vid             135\n",
      "Cid               0\n",
      "RepliesToId    5982\n",
      "Comments          0\n",
      "uniqueId          4\n",
      "videoWebUrl       0\n",
      "dtype: int64\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "['comments']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14364\\3859847329.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# Check missing values in comments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomments_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m# Remove missing values or fill with placeholder text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mcomments_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'comments'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# Alternatively, to fill:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# comments_df['comments'].fillna(\"No comment provided\", inplace=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001b[0m\n\u001b[0;32m   6666\u001b[0m             \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magg_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6667\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6668\u001b[0m             \u001b[0mcheck\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6669\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcheck\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6670\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6671\u001b[0m             \u001b[0magg_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0magg_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6672\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6673\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mthresh\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_default\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ['comments']"
     ]
    }
   ],
   "source": [
    "#Data Preprocessing And Cleaning\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#Step 1: Environment Setup\n",
    "!pip install pandas numpy openpyxl keybert transformers torch scikit-learn\n",
    "\n",
    "\n",
    "# Loading the Merged Comment data\n",
    "comments_df = pd.read_csv(\"G:/Other computers/My Laptop (1)/Pelajaran UiTM/Classes and Lectures (Semester 5)/FINAL YEAR PROJECT/Developments/Simplified Data/Merged Files/Merged Comments V3.csv\")\n",
    "\n",
    "# Loading the Video with Transcription data\n",
    "videos_df = pd.read_excel(\"G:/Other computers/My Laptop (1)/Pelajaran UiTM/Classes and Lectures (Semester 5)/FINAL YEAR PROJECT/Developments/Simplified Data/Transcriptions/Video with Transcription.xlsx\")\n",
    "\n",
    "#Inspecting Data\n",
    "comments_df.head()\n",
    "videos_df.head()\n",
    "\n",
    "# Step 2: Handling Missing Values Section\n",
    "# Check missing values in comments\n",
    "print(comments_df.isnull().sum())\n",
    "\n",
    "# Remove missing values or fill with placeholder text\n",
    "comments_df.dropna(subset=['comments'], inplace=True)\n",
    "\n",
    "# Alternatively, to fill:\n",
    "# comments_df['comments'].fillna(\"No comment provided\", inplace=True)\n",
    "\n",
    "# Check missing values in videos\n",
    "print(videos_df.isnull().sum())\n",
    "\n",
    "videos_df.dropna(subset=['transcription'], inplace=True)\n",
    "\n",
    "# Step 3: Removing duplicates Section\n",
    "# Remove duplicates from comments\n",
    "comments_df.drop_duplicates(subset=['comments'], inplace=True)\n",
    "\n",
    "# Remove duplicates from videos\n",
    "videos_df.drop_duplicates(subset=['transcription'], inplace=True)\n",
    "\n",
    "# Step4: Text Normalization & Cleaning\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()  # convert text to lowercase\n",
    "    text = re.sub(r'http\\S+', '', text)  # remove URLs\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # normalize whitespace\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to comments and transcriptions\n",
    "comments_df['clean_comments'] = comments_df['comments'].apply(clean_text)\n",
    "videos_df['clean_transcriptions'] = videos_df['transcription'].apply(clean_text)\n",
    "\n",
    "#Step 5: Tokenization\n",
    "# Tokenize the cleaned comments and transcriptions\n",
    "comments_df['tokens'] = comments_df['clean_comments'].apply(word_tokenize)\n",
    "videos_df['tokens'] = videos_df['clean_transcriptions'].apply(word_tokenize)\n",
    "\n",
    "# Step 6: Stopword Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define function for removing stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "comments_df['tokens_no_stopwords'] = comments_df['tokens'].apply(remove_stopwords)\n",
    "videos_df['tokens_no_stopwords'] = videos_df['tokens'].apply(remove_stopwords)\n",
    "\n",
    "#Step 7: Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "comments_df['lemmatized'] = comments_df['tokens_no_stopwords'].apply(lemmatize_tokens)\n",
    "videos_df['lemmatized'] = videos_df['tokens_no_stopwords'].apply(lemmatize_tokens)\n",
    "\n",
    "#Step 8: Preparing for Keyword Extraction (KeyBERT)\n",
    "comments_df['final_text'] = comments_df['lemmatized'].apply(lambda x: ' '.join(x))\n",
    "videos_df['final_text'] = videos_df['lemmatized'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "#Step 9: Save Preprocessed Data\n",
    "comments_df.to_csv('clean_comments.csv', index=False)\n",
    "videos_df.to_csv('clean_transcriptions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4753772a-ce8f-4e49-b156-eeb2eee0ff71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'punkt' already exists.\n",
      "✅ 'stopwords' already exists.\n",
      "⏳ Downloading 'wordnet'...\n",
      "⏳ Downloading 'omw-1.4'...\n",
      "\n",
      "🔍 Verifying downloads...\n",
      "✅ 'punkt' successfully downloaded and verified.\n",
      "✅ 'stopwords' successfully downloaded and verified.\n",
      "❌ 'wordnet' download failed. Please try manually.\n",
      "❌ 'omw-1.4' download failed. Please try manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#STEP 9\n",
    "#Installing NLTK\n",
    "#Pass this point is successful one\n",
    "\n",
    "import nltk\n",
    "\n",
    "# List of essential NLTK resources\n",
    "resources = [\n",
    "    ('punkt', 'tokenizers/punkt'),\n",
    "    ('stopwords', 'corpora/stopwords'),\n",
    "    ('wordnet', 'corpora/wordnet'),\n",
    "    ('omw-1.4', 'corpora/omw-1.4')  # Optional, for extended lemmatization support\n",
    "]\n",
    "\n",
    "for resource_name, resource_path in resources:\n",
    "    try:\n",
    "        nltk.data.find(resource_path)\n",
    "        print(f\"✅ '{resource_name}' already exists.\")\n",
    "    except LookupError:\n",
    "        print(f\"⏳ Downloading '{resource_name}'...\")\n",
    "        nltk.download(resource_name)\n",
    "\n",
    "# Verification\n",
    "print(\"\\n🔍 Verifying downloads...\")\n",
    "for resource_name, resource_path in resources:\n",
    "    try:\n",
    "        nltk.data.find(resource_path)\n",
    "        print(f\"✅ '{resource_name}' successfully downloaded and verified.\")\n",
    "    except LookupError:\n",
    "        print(f\"❌ '{resource_name}' download failed. Please try manually.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f414861-70e8-4e0b-894d-6b80b3671eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\User/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to ./nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to ./nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-downloading NLTK resources...\n"
     ]
    }
   ],
   "source": [
    "#STEPS 10\n",
    "#KeyBERT Implementation \n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, ttk, messagebox, scrolledtext\n",
    "import threading\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keybert import KeyBERT\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet', download_dir='./nltk_data', quiet=False)\n",
    "nltk.download('omw-1.4', download_dir='./nltk_data', quiet=False)\n",
    "\n",
    "class KeywordExtractionApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Keyword Extraction Tool\")\n",
    "        self.root.geometry(\"800x600\")\n",
    "        self.root.resizable(True, True)\n",
    "        \n",
    "        # Initialize variables\n",
    "        self.comments_file_path = tk.StringVar()\n",
    "        self.videos_file_path = tk.StringVar()\n",
    "        self.output_dir = tk.StringVar()\n",
    "        self.output_dir.set(os.getcwd())\n",
    "        self.progress_var = tk.DoubleVar()\n",
    "        self.status_var = tk.StringVar()\n",
    "        self.status_var.set(\"Ready\")\n",
    "        self.processing_thread = None\n",
    "        self.stop_requested = False\n",
    "        \n",
    "        # Configure the main frame\n",
    "        main_frame = ttk.Frame(root, padding=\"10\")\n",
    "        main_frame.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # Create the input file selection section\n",
    "        self.create_file_selection_section(main_frame)\n",
    "        \n",
    "        # Create the settings section\n",
    "        self.create_settings_section(main_frame)\n",
    "        \n",
    "        # Create the output section\n",
    "        self.create_output_section(main_frame)\n",
    "        \n",
    "        # Create the log section\n",
    "        self.create_log_section(main_frame)\n",
    "        \n",
    "        # Create the control buttons section\n",
    "        self.create_control_buttons(main_frame)\n",
    "        \n",
    "        # Configure progress bar\n",
    "        self.progress_bar = ttk.Progressbar(main_frame, variable=self.progress_var, length=780)\n",
    "        self.progress_bar.pack(fill=tk.X, padx=10, pady=5)\n",
    "        \n",
    "        # Configure status bar\n",
    "        status_bar = ttk.Label(main_frame, textvariable=self.status_var, relief=tk.SUNKEN, anchor=tk.W)\n",
    "        status_bar.pack(fill=tk.X, padx=10, pady=5)\n",
    "        \n",
    "    def create_file_selection_section(self, parent):\n",
    "        file_frame = ttk.LabelFrame(parent, text=\"Input Files\", padding=\"10\")\n",
    "        file_frame.pack(fill=tk.X, padx=10, pady=5)\n",
    "        \n",
    "        # Comments file selector\n",
    "        ttk.Label(file_frame, text=\"Comments File:\").grid(row=0, column=0, sticky=tk.W, padx=5, pady=5)\n",
    "        ttk.Entry(file_frame, textvariable=self.comments_file_path, width=60).grid(row=0, column=1, padx=5, pady=5)\n",
    "        ttk.Button(file_frame, text=\"Browse...\", command=self.browse_comments_file).grid(row=0, column=2, padx=5, pady=5)\n",
    "        \n",
    "        # Videos file selector\n",
    "        ttk.Label(file_frame, text=\"Videos File:\").grid(row=1, column=0, sticky=tk.W, padx=5, pady=5)\n",
    "        ttk.Entry(file_frame, textvariable=self.videos_file_path, width=60).grid(row=1, column=1, padx=5, pady=5)\n",
    "        ttk.Button(file_frame, text=\"Browse...\", command=self.browse_videos_file).grid(row=1, column=2, padx=5, pady=5)\n",
    "        \n",
    "    def create_settings_section(self, parent):\n",
    "        settings_frame = ttk.LabelFrame(parent, text=\"Processing Settings\", padding=\"10\")\n",
    "        settings_frame.pack(fill=tk.X, padx=10, pady=5)\n",
    "        \n",
    "        # Comment column name\n",
    "        ttk.Label(settings_frame, text=\"Comment Column:\").grid(row=0, column=0, sticky=tk.W, padx=5, pady=5)\n",
    "        self.comment_column_var = tk.StringVar(value=\"Comments\")\n",
    "        ttk.Entry(settings_frame, textvariable=self.comment_column_var, width=20).grid(row=0, column=1, padx=5, pady=5)\n",
    "        \n",
    "        # Transcription column name\n",
    "        ttk.Label(settings_frame, text=\"Transcription Column:\").grid(row=0, column=2, sticky=tk.W, padx=5, pady=5)\n",
    "        self.transcription_column_var = tk.StringVar(value=\"transcription\")\n",
    "        ttk.Entry(settings_frame, textvariable=self.transcription_column_var, width=20).grid(row=0, column=3, padx=5, pady=5)\n",
    "        \n",
    "        # Top N keywords\n",
    "        ttk.Label(settings_frame, text=\"Number of Keywords:\").grid(row=1, column=0, sticky=tk.W, padx=5, pady=5)\n",
    "        self.top_n_var = tk.IntVar(value=5)\n",
    "        ttk.Spinbox(settings_frame, from_=1, to=20, textvariable=self.top_n_var, width=5).grid(row=1, column=1, padx=5, pady=5)\n",
    "        \n",
    "        # Batch size\n",
    "        ttk.Label(settings_frame, text=\"Batch Size:\").grid(row=1, column=2, sticky=tk.W, padx=5, pady=5)\n",
    "        self.batch_size_var = tk.IntVar(value=100)\n",
    "        ttk.Spinbox(settings_frame, from_=10, to=500, textvariable=self.batch_size_var, width=5).grid(row=1, column=3, padx=5, pady=5)\n",
    "        \n",
    "    def create_output_section(self, parent):\n",
    "        output_frame = ttk.LabelFrame(parent, text=\"Output Settings\", padding=\"10\")\n",
    "        output_frame.pack(fill=tk.X, padx=10, pady=5)\n",
    "        \n",
    "        # Output directory selector\n",
    "        ttk.Label(output_frame, text=\"Output Directory:\").grid(row=0, column=0, sticky=tk.W, padx=5, pady=5)\n",
    "        ttk.Entry(output_frame, textvariable=self.output_dir, width=60).grid(row=0, column=1, padx=5, pady=5)\n",
    "        ttk.Button(output_frame, text=\"Browse...\", command=self.browse_output_dir).grid(row=0, column=2, padx=5, pady=5)\n",
    "        \n",
    "    def create_log_section(self, parent):\n",
    "        log_frame = ttk.LabelFrame(parent, text=\"Processing Log\", padding=\"10\")\n",
    "        log_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)\n",
    "        \n",
    "        # Log text area\n",
    "        self.log_text = scrolledtext.ScrolledText(log_frame, wrap=tk.WORD, width=80, height=10)\n",
    "        self.log_text.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        self.log_text.config(state=tk.DISABLED)\n",
    "        \n",
    "    def create_control_buttons(self, parent):\n",
    "        button_frame = ttk.Frame(parent)\n",
    "        button_frame.pack(fill=tk.X, padx=10, pady=5)\n",
    "        \n",
    "        # Control buttons\n",
    "        self.start_button = ttk.Button(button_frame, text=\"Start Processing\", command=self.start_processing)\n",
    "        self.start_button.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        self.stop_button = ttk.Button(button_frame, text=\"Stop\", command=self.stop_processing, state=tk.DISABLED)\n",
    "        self.stop_button.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        ttk.Button(button_frame, text=\"Exit\", command=self.root.destroy).pack(side=tk.RIGHT, padx=5)\n",
    "        \n",
    "    def browse_comments_file(self):\n",
    "        filepath = filedialog.askopenfilename(\n",
    "            filetypes=[(\"CSV files\", \"*.csv\"), (\"Excel files\", \"*.xlsx;*.xls\"), (\"All files\", \"*.*\")]\n",
    "        )\n",
    "        if filepath:\n",
    "            self.comments_file_path.set(filepath)\n",
    "            \n",
    "    def browse_videos_file(self):\n",
    "        filepath = filedialog.askopenfilename(\n",
    "            filetypes=[(\"Excel files\", \"*.xlsx;*.xls\"), (\"CSV files\", \"*.csv\"), (\"All files\", \"*.*\")]\n",
    "        )\n",
    "        if filepath:\n",
    "            self.videos_file_path.set(filepath)\n",
    "            \n",
    "    def browse_output_dir(self):\n",
    "        dirpath = filedialog.askdirectory()\n",
    "        if dirpath:\n",
    "            self.output_dir.set(dirpath)\n",
    "            \n",
    "    def log(self, message):\n",
    "        self.root.after(0, self._log, message)\n",
    "        \n",
    "    def _log(self, message):\n",
    "        self.log_text.config(state=tk.NORMAL)\n",
    "        self.log_text.insert(tk.END, f\"{message}\\n\")\n",
    "        self.log_text.see(tk.END)\n",
    "        self.log_text.config(state=tk.DISABLED)\n",
    "        \n",
    "    def update_status(self, message):\n",
    "        self.status_var.set(message)\n",
    "        \n",
    "    def update_progress(self, value):\n",
    "        self.progress_var.set(value)\n",
    "        \n",
    "    def start_processing(self):\n",
    "        # Check if input files are provided\n",
    "        if not self.comments_file_path.get() and not self.videos_file_path.get():\n",
    "            messagebox.showerror(\"Error\", \"Please select at least one input file.\")\n",
    "            return\n",
    "            \n",
    "        # Disable start button and enable stop button\n",
    "        self.start_button.config(state=tk.DISABLED)\n",
    "        self.stop_button.config(state=tk.NORMAL)\n",
    "        \n",
    "        # Reset stop flag\n",
    "        self.stop_requested = False\n",
    "        \n",
    "        # Clear log\n",
    "        self.log_text.config(state=tk.NORMAL)\n",
    "        self.log_text.delete(1.0, tk.END)\n",
    "        self.log_text.config(state=tk.DISABLED)\n",
    "        \n",
    "        # Start processing in a separate thread\n",
    "        self.processing_thread = threading.Thread(target=self.process_files)\n",
    "        self.processing_thread.daemon = True\n",
    "        self.processing_thread.start()\n",
    "        \n",
    "    def stop_processing(self):\n",
    "        self.stop_requested = True\n",
    "        self.log(\"Stop requested. Waiting for current batch to complete...\")\n",
    "        self.update_status(\"Stopping...\")\n",
    "        \n",
    "    def process_files(self):\n",
    "        try:\n",
    "            self.log(\"Starting keyword extraction process...\")\n",
    "            self.update_status(\"Initializing...\")\n",
    "            \n",
    "            # Set parameters\n",
    "            comment_column = self.comment_column_var.get()\n",
    "            transcription_column = self.transcription_column_var.get()\n",
    "            top_n = self.top_n_var.get()\n",
    "            batch_size = self.batch_size_var.get()\n",
    "            output_dir = self.output_dir.get()\n",
    "            \n",
    "            # Download NLTK resources with better error handling\n",
    "            self.log(\"Downloading NLTK resources...\")\n",
    "            self.update_status(\"Downloading NLTK resources...\")\n",
    "            \n",
    "            # Ensure NLTK data directory exists\n",
    "            import os\n",
    "            nltk_data_dir = os.path.expanduser(\"~/nltk_data\")\n",
    "            if not os.path.exists(nltk_data_dir):\n",
    "                os.makedirs(nltk_data_dir, exist_ok=True)\n",
    "                \n",
    "            # Download resources with download_dir specified and explicit downloading\n",
    "            try:\n",
    "                self.log(\"Downloading punkt tokenizer...\")\n",
    "                nltk.download('punkt', download_dir=nltk_data_dir, quiet=False)\n",
    "                \n",
    "                self.log(\"Downloading stopwords...\")\n",
    "                nltk.download('stopwords', download_dir=nltk_data_dir, quiet=False)\n",
    "                \n",
    "                self.log(\"Downloading wordnet...\")\n",
    "                nltk.download('wordnet', download_dir=nltk_data_dir, quiet=False)\n",
    "                \n",
    "                # Verify the downloads\n",
    "                from nltk.data import find\n",
    "                try:\n",
    "                    find('tokenizers/punkt')\n",
    "                    find('corpora/stopwords')\n",
    "                    find('corpora/wordnet')\n",
    "                    self.log(\"All NLTK resources successfully downloaded and verified.\")\n",
    "                except LookupError as e:\n",
    "                    self.log(f\"Error verifying NLTK resources: {e}\")\n",
    "                    self.log(\"Attempting alternative download method...\")\n",
    "                    # Try alternative download method\n",
    "                    import subprocess\n",
    "                    python_executable = sys.executable\n",
    "                    subprocess.call([python_executable, '-m', 'nltk.downloader', 'punkt', 'stopwords', 'wordnet'])\n",
    "                    self.log(\"Alternative download completed. Continuing with processing...\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                self.log(f\"Error downloading NLTK resources: {e}\")\n",
    "                messagebox.showwarning(\"NLTK Download Warning\", \n",
    "                                      \"There was an issue downloading NLTK resources. You may need to download them manually.\\n\\n\"\n",
    "                                      \"Try running these commands in your Python environment:\\n\"\n",
    "                                      \"import nltk\\n\"\n",
    "                                      \"nltk.download('punkt')\\n\"\n",
    "                                      \"nltk.download('stopwords')\\n\"\n",
    "                                      \"nltk.download('wordnet')\")\n",
    "            \n",
    "            # Load data\n",
    "            comments_df = None\n",
    "            videos_df = None\n",
    "            \n",
    "            if self.comments_file_path.get():\n",
    "                self.log(f\"Loading comments data from {self.comments_file_path.get()}...\")\n",
    "                self.update_status(\"Loading comments data...\")\n",
    "                if self.comments_file_path.get().lower().endswith('.csv'):\n",
    "                    comments_df = pd.read_csv(self.comments_file_path.get())\n",
    "                else:\n",
    "                    comments_df = pd.read_excel(self.comments_file_path.get())\n",
    "                self.log(f\"Successfully loaded comments data with {len(comments_df)} rows\")\n",
    "                self.log(f\"Columns in comments_df: {comments_df.columns.tolist()}\")\n",
    "                \n",
    "            if self.videos_file_path.get():\n",
    "                self.log(f\"Loading videos data from {self.videos_file_path.get()}...\")\n",
    "                self.update_status(\"Loading videos data...\")\n",
    "                if self.videos_file_path.get().lower().endswith('.csv'):\n",
    "                    videos_df = pd.read_csv(self.videos_file_path.get())\n",
    "                else:\n",
    "                    videos_df = pd.read_excel(self.videos_file_path.get())\n",
    "                self.log(f\"Successfully loaded videos data with {len(videos_df)} rows\")\n",
    "                self.log(f\"Columns in videos_df: {videos_df.columns.tolist()}\")\n",
    "                \n",
    "            # Check if the specified columns exist\n",
    "            if comments_df is not None and comment_column not in comments_df.columns:\n",
    "                self.log(f\"Warning: '{comment_column}' column not found in comments DataFrame\")\n",
    "                self.log(f\"Available columns: {comments_df.columns.tolist()}\")\n",
    "                potential_columns = [col for col in comments_df.columns if 'comment' in col.lower()]\n",
    "                if potential_columns:\n",
    "                    comment_column = potential_columns[0]\n",
    "                    self.log(f\"Using '{comment_column}' as comment column instead\")\n",
    "                else:\n",
    "                    comments_df = None\n",
    "                    self.log(\"No suitable comment column found. Skipping comments processing.\")\n",
    "                \n",
    "            if videos_df is not None and transcription_column not in videos_df.columns:\n",
    "                self.log(f\"Warning: '{transcription_column}' column not found in videos DataFrame\")\n",
    "                self.log(f\"Available columns: {videos_df.columns.tolist()}\")\n",
    "                potential_columns = [col for col in videos_df.columns if 'transcript' in col.lower() or 'text' in col.lower()]\n",
    "                if potential_columns:\n",
    "                    transcription_column = potential_columns[0]\n",
    "                    self.log(f\"Using '{transcription_column}' as transcription column instead\")\n",
    "                else:\n",
    "                    videos_df = None\n",
    "                    self.log(\"No suitable transcription column found. Skipping videos processing.\")\n",
    "            \n",
    "            # Text cleaning and processing functions\n",
    "            self.log(\"Setting up text processing functions...\")\n",
    "            \n",
    "            def clean_text(text):\n",
    "                if not isinstance(text, str):\n",
    "                    return \"\"\n",
    "                text = text.lower()\n",
    "                text = re.sub(r'http\\S+', '', text)\n",
    "                text = re.sub(r'[^\\w\\s]', '', text)\n",
    "                text = re.sub(r'\\d+', '', text)\n",
    "                text = re.sub(r'\\s+', ' ', text).strip()\n",
    "                return text\n",
    "            \n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            \n",
    "            def process_text(text):\n",
    "                tokens = word_tokenize(text)\n",
    "                tokens_no_stopwords = [word for word in tokens if word not in stop_words]\n",
    "                lemmatized = [lemmatizer.lemmatize(token) for token in tokens_no_stopwords]\n",
    "                return ' '.join(lemmatized)\n",
    "            \n",
    "            # Initialize KeyBERT\n",
    "            self.log(\"Initializing KeyBERT model...\")\n",
    "            self.update_status(\"Initializing KeyBERT model...\")\n",
    "            kw_model = KeyBERT()\n",
    "            \n",
    "            # Function to extract keywords\n",
    "            def extract_keywords(text, top_n=5):\n",
    "                if not text or len(text.split()) < 3:\n",
    "                    return []\n",
    "                try:\n",
    "                    keywords = kw_model.extract_keywords(\n",
    "                        text,\n",
    "                        keyphrase_ngram_range=(1, 2),\n",
    "                        stop_words='english',\n",
    "                        use_mmr=True,\n",
    "                        diversity=0.7,\n",
    "                        top_n=top_n\n",
    "                    )\n",
    "                    return keywords\n",
    "                except Exception as e:\n",
    "                    self.log(f\"Error extracting keywords: {e}\")\n",
    "                    return []\n",
    "            \n",
    "            # Process comments if available\n",
    "            if comments_df is not None:\n",
    "                self.log(\"\\nProcessing comments data...\")\n",
    "                self.update_status(\"Processing comments data...\")\n",
    "                \n",
    "                # Clean and preprocess text\n",
    "                self.log(\"Cleaning and preprocessing comments...\")\n",
    "                comments_df['clean_text'] = comments_df[comment_column].apply(clean_text)\n",
    "                comments_df['processed_text'] = comments_df['clean_text'].apply(process_text)\n",
    "                \n",
    "                # Extract keywords in batches\n",
    "                self.log(\"Extracting keywords from comments...\")\n",
    "                keywords_list = []\n",
    "                total_batches = (len(comments_df) + batch_size - 1) // batch_size\n",
    "                \n",
    "                for i in range(0, len(comments_df), batch_size):\n",
    "                    if self.stop_requested:\n",
    "                        self.log(\"Processing stopped by user.\")\n",
    "                        break\n",
    "                    \n",
    "                    batch_end = min(i + batch_size, len(comments_df))\n",
    "                    self.log(f\"Processing comment batch {i+1} to {batch_end} of {len(comments_df)}\")\n",
    "                    self.update_status(f\"Processing comments: {batch_end}/{len(comments_df)}\")\n",
    "                    \n",
    "                    batch = comments_df['processed_text'].iloc[i:batch_end]\n",
    "                    batch_keywords = [extract_keywords(text, top_n) for text in batch]\n",
    "                    keywords_list.extend(batch_keywords)\n",
    "                    \n",
    "                    # Update progress\n",
    "                    progress = (batch_end / len(comments_df)) * 50  # First 50% of progress\n",
    "                    self.update_progress(progress)\n",
    "                    \n",
    "                # Add keywords to dataframe\n",
    "                if not self.stop_requested:\n",
    "                    comments_df['keywords'] = keywords_list\n",
    "                    comments_df['keywords_only'] = comments_df['keywords'].apply(\n",
    "                        lambda kw_list: [k for k, _ in kw_list] if isinstance(kw_list, list) else []\n",
    "                    )\n",
    "                    \n",
    "                    # Save processed comments\n",
    "                    output_path = os.path.join(output_dir, 'comments_with_keywords.csv')\n",
    "                    self.log(f\"Saving processed comments to {output_path}\")\n",
    "                    comments_df.to_csv(output_path, index=False)\n",
    "                    self.log(f\"Saved comments with keywords to {output_path}\")\n",
    "                    \n",
    "                    # Display sample results\n",
    "                    self.log(\"\\nSample keywords from comments:\")\n",
    "                    for i in range(min(3, len(comments_df))):\n",
    "                        self.log(f\"Comment: {comments_df[comment_column].iloc[i][:100]}...\")\n",
    "                        self.log(f\"Keywords: {comments_df['keywords'].iloc[i]}\")\n",
    "                        self.log(\"\")\n",
    "            \n",
    "            # Process videos if available\n",
    "            if videos_df is not None and not self.stop_requested:\n",
    "                self.log(\"\\nProcessing video transcription data...\")\n",
    "                self.update_status(\"Processing video transcriptions...\")\n",
    "                \n",
    "                # Clean and preprocess text\n",
    "                self.log(\"Cleaning and preprocessing transcriptions...\")\n",
    "                videos_df['clean_text'] = videos_df[transcription_column].apply(clean_text)\n",
    "                videos_df['processed_text'] = videos_df['clean_text'].apply(process_text)\n",
    "                \n",
    "                # Extract keywords in batches\n",
    "                self.log(\"Extracting keywords from transcriptions...\")\n",
    "                keywords_list = []\n",
    "                total_batches = (len(videos_df) + batch_size - 1) // batch_size\n",
    "                \n",
    "                for i in range(0, len(videos_df), batch_size):\n",
    "                    if self.stop_requested:\n",
    "                        self.log(\"Processing stopped by user.\")\n",
    "                        break\n",
    "                    \n",
    "                    batch_end = min(i + batch_size, len(videos_df))\n",
    "                    self.log(f\"Processing transcription batch {i+1} to {batch_end} of {len(videos_df)}\")\n",
    "                    self.update_status(f\"Processing transcriptions: {batch_end}/{len(videos_df)}\")\n",
    "                    \n",
    "                    batch = videos_df['processed_text'].iloc[i:batch_end]\n",
    "                    batch_keywords = [extract_keywords(text, top_n) for text in batch]\n",
    "                    keywords_list.extend(batch_keywords)\n",
    "                    \n",
    "                    # Update progress\n",
    "                    base_progress = 50 if comments_df is not None else 0\n",
    "                    progress = base_progress + (batch_end / len(videos_df)) * (100 - base_progress)\n",
    "                    self.update_progress(progress)\n",
    "                    \n",
    "                # Add keywords to dataframe\n",
    "                if not self.stop_requested:\n",
    "                    videos_df['keywords'] = keywords_list\n",
    "                    videos_df['keywords_only'] = videos_df['keywords'].apply(\n",
    "                        lambda kw_list: [k for k, _ in kw_list] if isinstance(kw_list, list) else []\n",
    "                    )\n",
    "                    \n",
    "                    # Save processed videos\n",
    "                    output_path = os.path.join(output_dir, 'videos_with_keywords.csv')\n",
    "                    self.log(f\"Saving processed videos to {output_path}\")\n",
    "                    videos_df.to_csv(output_path, index=False)\n",
    "                    self.log(f\"Saved videos with keywords to {output_path}\")\n",
    "                    \n",
    "                    # Display sample results\n",
    "                    self.log(\"\\nSample keywords from videos:\")\n",
    "                    for i in range(min(3, len(videos_df))):\n",
    "                        self.log(f\"Transcription: {videos_df[transcription_column].iloc[i][:100]}...\")\n",
    "                        self.log(f\"Keywords: {videos_df['keywords'].iloc[i]}\")\n",
    "                        self.log(\"\")\n",
    "            \n",
    "            # Completion message\n",
    "            if self.stop_requested:\n",
    "                self.update_status(\"Processing stopped by user\")\n",
    "                self.log(\"Processing stopped by user before completion.\")\n",
    "            else:\n",
    "                self.update_status(\"Processing completed\")\n",
    "                self.log(\"Keyword extraction completed successfully!\")\n",
    "                self.update_progress(100)\n",
    "                messagebox.showinfo(\"Success\", \"Keyword extraction completed successfully!\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.log(f\"Error during processing: {e}\")\n",
    "            import traceback\n",
    "            self.log(traceback.format_exc())\n",
    "            self.update_status(\"Error occurred\")\n",
    "            messagebox.showerror(\"Error\", f\"An error occurred during processing: {str(e)}\")\n",
    "        \n",
    "        finally:\n",
    "            # Re-enable start button and disable stop button\n",
    "            self.root.after(0, lambda: self.start_button.config(state=tk.NORMAL))\n",
    "            self.root.after(0, lambda: self.stop_button.config(state=tk.DISABLED))\n",
    "\n",
    "# Main application launcher\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    # Check if required packages are installed\n",
    "    try:\n",
    "        import nltk\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from keybert import KeyBERT\n",
    "        \n",
    "        # Pre-download NLTK resources before GUI starts\n",
    "        print(\"Pre-downloading NLTK resources...\")\n",
    "        nltk_data_dir = os.path.expanduser(\"~/nltk_data\")\n",
    "        if not os.path.exists(nltk_data_dir):\n",
    "            os.makedirs(nltk_data_dir, exist_ok=True)\n",
    "            \n",
    "        # Function to check if resource exists\n",
    "        def resource_exists(resource_path):\n",
    "            try:\n",
    "                from nltk.data import find\n",
    "                find(resource_path)\n",
    "                return True\n",
    "            except LookupError:\n",
    "                return False\n",
    "        \n",
    "        # Download resources if they don't exist\n",
    "        if not resource_exists('tokenizers/punkt'):\n",
    "            print(\"Downloading punkt tokenizer...\")\n",
    "            nltk.download('punkt', download_dir=nltk_data_dir, quiet=False)\n",
    "            \n",
    "        if not resource_exists('corpora/stopwords'):\n",
    "            print(\"Downloading stopwords...\")\n",
    "            nltk.download('stopwords', download_dir=nltk_data_dir, quiet=False)\n",
    "            \n",
    "        if not resource_exists('corpora/wordnet'):\n",
    "            print(\"Downloading wordnet...\")\n",
    "            nltk.download('wordnet', download_dir=nltk_data_dir, quiet=False)\n",
    "            \n",
    "    except ImportError as e:\n",
    "        print(f\"Missing required package: {e}\")\n",
    "        print(\"Please install the required packages using:\")\n",
    "        print(\"pip install pandas numpy nltk keybert transformers torch scikit-learn openpyxl\")\n",
    "        messagebox.showerror(\"Missing Packages\", \n",
    "                            \"Some required packages are missing. Please install them using:\\n\\n\"\n",
    "                            \"pip install pandas numpy nltk keybert transformers torch scikit-learn openpyxl\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Create and run the application\n",
    "    root = tk.Tk()\n",
    "    app = KeywordExtractionApp(root)\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "475d501f-8894-43bf-9574-f1e7b4e93dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm feeling down\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Rank keywords separately for videos and comments\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m video_ranks \u001b[38;5;241m=\u001b[39m rank_keywords(videos_df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m\"\u001b[39m, user_input)\n\u001b[0;32m     59\u001b[0m comment_ranks \u001b[38;5;241m=\u001b[39m rank_keywords(comments_df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m\"\u001b[39m, user_input)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Convert to DataFrame and display\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 41\u001b[0m, in \u001b[0;36mrank_keywords\u001b[1;34m(df, keyword_column, user_input)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m df[keyword_column]\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39munique():\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 41\u001b[0m         keyword_embedding \u001b[38;5;241m=\u001b[39m get_embedding(keyword)\n\u001b[0;32m     42\u001b[0m         score \u001b[38;5;241m=\u001b[39m cosine_similarity([user_embedding\u001b[38;5;241m.\u001b[39mnumpy()], [keyword_embedding\u001b[38;5;241m.\u001b[39mnumpy()])[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     43\u001b[0m         rankings\u001b[38;5;241m.\u001b[39mappend((keyword, score))\n",
      "Cell \u001b[1;32mIn[8], line 31\u001b[0m, in \u001b[0;36mget_embedding\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     29\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1143\u001b[0m     embedding_output,\n\u001b[0;32m   1144\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   1145\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1146\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1147\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m   1148\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1149\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1150\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1151\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1152\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1153\u001b[0m )\n\u001b[0;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    686\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m         output_attentions,\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    696\u001b[0m         hidden_states,\n\u001b[0;32m    697\u001b[0m         attention_mask,\n\u001b[0;32m    698\u001b[0m         layer_head_mask,\n\u001b[0;32m    699\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    700\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    701\u001b[0m         past_key_value,\n\u001b[0;32m    702\u001b[0m         output_attentions,\n\u001b[0;32m    703\u001b[0m     )\n\u001b[0;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[0;32m    586\u001b[0m         hidden_states,\n\u001b[0;32m    587\u001b[0m         attention_mask,\n\u001b[0;32m    588\u001b[0m         head_mask,\n\u001b[0;32m    589\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    590\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mself_attn_past_key_value,\n\u001b[0;32m    591\u001b[0m     )\n\u001b[0;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:515\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[0;32m    516\u001b[0m         hidden_states,\n\u001b[0;32m    517\u001b[0m         attention_mask,\n\u001b[0;32m    518\u001b[0m         head_mask,\n\u001b[0;32m    519\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    520\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    521\u001b[0m         past_key_value,\n\u001b[0;32m    522\u001b[0m         output_attentions,\n\u001b[0;32m    523\u001b[0m     )\n\u001b[0;32m    524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:408\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    406\u001b[0m     key_layer, value_layer \u001b[38;5;241m=\u001b[39m past_key_value\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 408\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(current_states))\n\u001b[0;32m    409\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(current_states))\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#STEPS 11 - Implementing BERT model\n",
    "#Try Accuracy Testing MentalBERT\n",
    "\n",
    "# Install dependencies (uncomment in Colab or terminal)\n",
    "# !pip install transformers torch pandas scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "hf_token = \"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\" # Keep your token secure!\n",
    "\n",
    "# Load MentalBERT model and tokenizer using the token\n",
    "try:\n",
    "    # Try loading with the token argument\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"mental/mental-bert-base-uncased\", token=hf_token)\n",
    "    model = AutoModel.from_pretrained(\"mental/mental-bert-base-uncased\", token=hf_token)\n",
    "except TypeError:\n",
    "    # Fallback for older transformers versions that might not accept 'token' directly\n",
    "    # In this case, ensure you are logged in via CLI or notebook_login()\n",
    "    print(\"Token argument not accepted or login required. Ensure you are logged in via CLI or notebook_login().\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"mental/mental-bert-base-uncased\")\n",
    "    model = AutoModel.from_pretrained(\"mental/mental-bert-base-uncased\")\n",
    "\n",
    "\n",
    "# Function to generate embedding using MentalBERT\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "# Function to rank keywords against a user input\n",
    "def rank_keywords(df, keyword_column, user_input):\n",
    "    user_embedding = get_embedding(user_input)\n",
    "    rankings = []\n",
    "\n",
    "    for keyword in df[keyword_column].dropna().unique():\n",
    "        try:\n",
    "            keyword_embedding = get_embedding(keyword)\n",
    "            score = cosine_similarity([user_embedding.numpy()], [keyword_embedding.numpy()])[0][0]\n",
    "            rankings.append((keyword, score))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing keyword: {keyword} — {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return sorted(rankings, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Load your CSV files\n",
    "videos_df = pd.read_csv(\"G:/Other computers/My Laptop (1)/Pelajaran UiTM/Classes and Lectures (Semester 6)/CSP650/Developments/Simplified Data/Testings/comments_with_keywords.csv\")\n",
    "comments_df = pd.read_csv(\"G:/Other computers/My Laptop (1)/Pelajaran UiTM/Classes and Lectures (Semester 6)/CSP650/Developments/Simplified Data/Testings/videos_with_keywords.csv\")\n",
    "\n",
    "# Input from user\n",
    "user_input = \"I'm feeling down\"\n",
    "\n",
    "# Rank keywords separately for videos and comments\n",
    "video_ranks = rank_keywords(videos_df, \"keywords\", user_input)\n",
    "comment_ranks = rank_keywords(comments_df, \"keywords\", user_input)\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "video_results = pd.DataFrame(video_ranks, columns=[\"Keyword\", \"Similarity Score\"])\n",
    "comment_results = pd.DataFrame(comment_ranks, columns=[\"Keyword\", \"Similarity Score\"])\n",
    "\n",
    "print(\"\\nTop Video Keywords Relevant to Input:\")\n",
    "print(video_results.head(10))\n",
    "\n",
    "print(\"\\nTop Comment Keywords Relevant to Input:\")\n",
    "print(comment_results.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "916e6b84-919e-44c2-806c-a0841cd22ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\anaconda\\lib\\site-packages (4.51.1)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in d:\\anaconda\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\anaconda\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\anaconda\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\anaconda\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\anaconda\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (2024.6.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModel\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Your Hugging Face token\n",
    "hf_token = \"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\"  # Keep your token secure!\n",
    "\n",
    "# Load MentalBERT model and tokenizer using the token\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"mental/mental-bert-base-uncased\", token=hf_token)\n",
    "    model = AutoModel.from_pretrained(\"mental/mental-bert-base-uncased\", token=hf_token)\n",
    "except TypeError:\n",
    "    print(\"Token argument not accepted or login required. Ensure you are logged in via CLI or notebook_login().\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"mental/mental-bert-base-uncased\")\n",
    "    model = AutoModel.from_pretrained(\"mental/mental-bert-base-uncased\")\n",
    "\n",
    "# Function to generate embedding using MentalBERT\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "# Function to generate embeddings for a batch of texts\n",
    "def get_batch_embeddings(texts):\n",
    "    # Ensure all texts are strings before tokenizing\n",
    "    texts = [str(text) for text in texts]  # Convert all to strings\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "# Function to rank keywords against a user input with batch processing\n",
    "def rank_keywords_batch(df, keyword_column, user_input, batch_size=10):\n",
    "    user_embedding = get_embedding(user_input)\n",
    "    rankings = []\n",
    "    \n",
    "    # Process keywords in batches with tqdm progress bar\n",
    "    for i in tqdm(range(0, len(df[keyword_column]), batch_size), desc=\"Processing keywords\"):\n",
    "        keywords_batch = df[keyword_column].dropna().unique()[i:i+batch_size]\n",
    "        \n",
    "        # Convert all keywords to string before embedding\n",
    "        keywords_batch = [str(keyword) for keyword in keywords_batch]\n",
    "        \n",
    "        try:\n",
    "            keyword_embeddings = get_batch_embeddings(keywords_batch)\n",
    "            similarities = cosine_similarity([user_embedding.numpy()] * len(keywords_batch), keyword_embeddings.numpy())\n",
    "            \n",
    "            for keyword, score in zip(keywords_batch, similarities):\n",
    "                rankings.append((keyword, score[0]))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch of keywords: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return sorted(rankings, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Load your CSV files (make sure paths are correct)\n",
    "videos_df = pd.read_csv(\"G:/Other computers/My Laptop (1)/Pelajaran UiTM/Classes and Lectures (Semester 6)/CSP650/Developments/Simplified Data/Testings/comments_with_keywords.csv\")\n",
    "comments_df = pd.read_csv(\"G:/Other computers/My Laptop (1)/Pelajaran UiTM/Classes and Lectures (Semester 6)/CSP650/Developments/Simplified Data/Testings/videos_with_keywords.csv\")\n",
    "\n",
    "# Clean up the data (remove any rows with missing 'keywords')\n",
    "videos_df = videos_df.dropna(subset=[\"keywords\"])\n",
    "comments_df = comments_df.dropna(subset=[\"keywords\"])\n",
    "\n",
    "# Input from user\n",
    "user_input = \"I'm feeling down\"\n",
    "\n",
    "# Normalize embeddings before calculating cosine similarity\n",
    "normalized_user_embedding = normalize([user_embedding.numpy()])\n",
    "normalized_keyword_embeddings = normalize(keyword_embeddings)\n",
    "\n",
    "similarities = cosine_similarity(normalized_user_embedding, normalized_keyword_embeddings)\n",
    "\n",
    "# Rank keywords separately for videos and comments using batch processing\n",
    "# Function to rank keywords against a user input with batch processing\n",
    "def rank_keywords_batch(df, keyword_column, user_input, batch_size=10):\n",
    "    user_embedding = get_embedding(user_input)\n",
    "\n",
    "    rankings = []\n",
    "    \n",
    "    # Process keywords in batches with tqdm progress bar\n",
    "    for i in tqdm(range(0, len(df[keyword_column]), batch_size), desc=\"Processing keywords\"):\n",
    "        # Get the current batch of keywords\n",
    "        keywords_batch = df[keyword_column].dropna().unique()[i:i+batch_size]\n",
    "        \n",
    "        # Ensure the batch is not empty\n",
    "        if not keywords_batch:\n",
    "            continue\n",
    "        \n",
    "        # Convert all keywords to string before embedding\n",
    "        keywords_batch = [str(keyword) for keyword in keywords_batch]\n",
    "        \n",
    "        try:\n",
    "            keyword_embeddings = get_batch_embeddings(keywords_batch)\n",
    "            \n",
    "            # Move embeddings to CPU if necessary before calculating similarity\n",
    "            keyword_embeddings = keyword_embeddings.cpu().numpy()\n",
    "            \n",
    "            similarities = cosine_similarity([user_embedding.numpy()] * len(keywords_batch), keyword_embeddings)\n",
    "            \n",
    "            # Rank keywords based on similarity\n",
    "            for keyword, score in zip(keywords_batch, similarities):\n",
    "                rankings.append((keyword, score[0]))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch of keywords: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return sorted(rankings, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Convert results to DataFrame and display\n",
    "video_results = pd.DataFrame(video_ranks, columns=[\"Keyword\", \"Similarity Score\"])\n",
    "comment_results = pd.DataFrame(comment_ranks, columns=[\"Keyword\", \"Similarity Score\"])\n",
    "\n",
    "print(\"\\nTop Video Keywords Relevant to Input:\")\n",
    "print(video_results.head(10))\n",
    "\n",
    "print(\"\\nTop Comment Keywords Relevant to Input:\")\n",
    "print(comment_results.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fbd844-cf92-472e-b30a-32a0e941a758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
